[{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Model Comparison with bgmCompare","text":"function bgmCompare() extends bgm() independent-sample designs. estimates whether edge weights category thresholds differ across groups ordinal Markov random field (MRF). Posterior inclusion probabilities indicate plausible group difference exists given parameter. can converted Bayes factors hypothesis testing.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"adhd-dataset","dir":"Articles","previous_headings":"","what":"ADHD dataset","title":"Model Comparison with bgmCompare","text":"illustrate subset ADHD dataset included bgms.","code":"library(bgms)  ?ADHD data_adhd = ADHD[ADHD$group == 1, -1] data_adhd = data_adhd[, 1:5] data_no_adhd = ADHD[ADHD$group == 0, -1] data_no_adhd = data_no_adhd[, 1:5]"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"fitting-a-model","dir":"Articles","previous_headings":"","what":"Fitting a model","title":"Model Comparison with bgmCompare","text":"","code":"fit = bgmCompare(x = data_adhd, y = data_no_adhd, seed = 1234)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"posterior-summaries","dir":"Articles","previous_headings":"","what":"Posterior summaries","title":"Model Comparison with bgmCompare","text":"summary shows baseline effects group differences: can extract posterior means inclusion probabilities:","code":"summary(fit) #> Posterior summaries from Bayesian grouped MRF estimation (bgmCompare): #>  #> Category thresholds: #>      parameter   mean  mcse    sd    n_eff  Rhat #> 1    avoid (1) -2.652 0.007 0.393 2838.525 1.001 #> 2 closeatt (1) -2.258 0.007 0.376 3139.100 1.000 #> 3 distract (1) -0.496 0.007 0.316 2117.341 1.002 #> 4   forget (1) -1.589 0.006 0.326 2621.001 1.001 #> 5 instruct (1) -2.432 0.008 0.374 2215.373 1.002 #>  #> Pairwise interactions: #>           parameter   mean  mcse    sd    n_eff  Rhat #> 1    avoid-closeatt  0.965 0.012 0.456 1438.106 1.005 #> 2    avoid-distract  1.693 0.007 0.353 2930.081 1.001 #> 3      avoid-forget  0.495 0.009 0.384 1797.217 1.003 #> 4    avoid-instruct  0.397 0.010 0.455 2226.350 1.004 #> 5 closeatt-distract -0.252 0.007 0.397 2822.267 1.000 #> 6   closeatt-forget  0.143 0.006 0.310 3090.678 1.002 #> ... (use `summary(fit)$pairwise` to see full output) #>  #> Inclusion probabilities: #>                  parameter  mean    sd  mcse n0->0 n0->1 n1->0 n1->1 #>               avoid (main) 1.000 0.000           0     0     0  3999 #>  avoid-closeatt (pairwise) 0.794 0.404 0.012   535   289   288  2887 #>  avoid-distract (pairwise) 0.406 0.491 0.008  1486   888   889   736 #>    avoid-forget (pairwise) 0.841 0.365 0.011   402   233   232  3132 #>  avoid-instruct (pairwise) 0.988 0.109 0.004    31    17    17  3934 #>            closeatt (main) 1.000 0.000           0     0     0  3999 #>     n_eff  Rhat #>                 #>  1132.933 1.014 #>   3415.16     1 #>  1114.745 1.005 #>     873.5 1.069 #>                 #> ... (use `summary(fit)$indicator` to see full output) #> Note: NA values are suppressed in the print table. They occur when an indicator #> was constant (all 0 or all 1) across all iterations, so sd/mcse/n_eff/Rhat #> are undefined; `summary(fit)$indicator` still contains the NA values. #>  #> Group differences (main effects): #>            parameter   mean    sd mcse n_eff  Rhat #>     avoid (diff1; 1) -2.552 0.746            1.000 #>  closeatt (diff1; 1) -2.980 0.733            1.002 #>  distract (diff1; 1) -2.525 0.642            1.001 #>    forget (diff1; 1) -2.826 0.664            1.000 #>  instruct (diff1; 1) -2.325 0.861            1.001 #> Note: NA values are suppressed in the print table. They occur here when an #> indicator was zero across all iterations, so mcse/n_eff/Rhat are undefined; #> `summary(fit)$main_diff` still contains the NA values. #>  #> Group differences (pairwise effects): #>                  parameter   mean    sd  mcse    n_eff  Rhat #>     avoid-closeatt (diff1)  1.242 0.907 0.023 1497.972 1.007 #>     avoid-distract (diff1)  0.225 0.367 0.008 2038.950 1.001 #>       avoid-forget (diff1)  1.269 0.817 0.020 1594.521 1.004 #>     avoid-instruct (diff1) -2.743 0.992 0.024 1716.671 1.003 #>  closeatt-distract (diff1) -0.184 0.353 0.008 1833.834 1.000 #>    closeatt-forget (diff1)  0.160 0.320 0.009 1265.097 1.002 #> ... (use `summary(fit)$pairwise_diff` to see full output) #> Note: NA values are suppressed in the print table. They occur here when an #> indicator was zero across all iterations, so mcse/n_eff/Rhat are undefined; #> `summary(fit)$pairwise_diff` still contains the NA values. #>  #> Use `summary(fit)$<component>` to access full results. #> See the `easybgm` package for other summary and plotting tools. coef(fit) #> $main_effects_raw #>                baseline     diff1 #> avoid(c1)    -2.6519010 -2.552031 #> closeatt(c1) -2.2580927 -2.980312 #> distract(c1) -0.4957581 -2.524521 #> forget(c1)   -1.5890157 -2.826021 #> instruct(c1) -2.4321107 -2.325337 #>  #> $pairwise_effects_raw #>                     baseline      diff1 #> avoid-closeatt     0.9652965  1.2424158 #> avoid-distract     1.6926659  0.2247702 #> avoid-forget       0.4952467  1.2689214 #> avoid-instruct     0.3971079 -2.7426345 #> closeatt-distract -0.2521829 -0.1835933 #> closeatt-forget    0.1425580  0.1596400 #> closeatt-instruct  1.5642360  0.5923538 #> distract-forget    0.4036371  0.2192210 #> distract-instruct  1.2730105  1.2178521 #> forget-instruct    1.1264064  0.8077411 #>  #> $main_effects_groups #>                  group1    group2 #> avoid(c1)    -1.3758853 -3.927917 #> closeatt(c1) -0.7679368 -3.748249 #> distract(c1)  0.7665024 -1.758019 #> forget(c1)   -0.1760053 -3.002026 #> instruct(c1) -1.2694422 -3.594779 #>  #> $pairwise_effects_groups #>                        group1     group2 #> avoid-closeatt     0.34408863  1.5865044 #> avoid-distract     1.58028076  1.8050510 #> avoid-forget      -0.13921404  1.1297074 #> avoid-instruct     1.76842516 -0.9742094 #> closeatt-distract -0.16038624 -0.3439795 #> closeatt-forget    0.06273804  0.2223781 #> closeatt-instruct  1.26805907  1.8604129 #> distract-forget    0.29402657  0.5132476 #> distract-instruct  0.66408446  1.8819366 #> forget-instruct    0.72253590  1.5302770 #>  #> $indicators #>            avoid closeatt distract  forget instruct #> avoid    1.00000  0.79400  0.40625 0.84125  0.98800 #> closeatt 0.79400  1.00000  0.38775 0.37100  0.58625 #> distract 0.40625  0.38775  1.00000 0.39675  0.83975 #> forget   0.84125  0.37100  0.39675 1.00000  0.72650 #> instruct 0.98800  0.58625  0.83975 0.72650  1.00000"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"visualizing-group-networks","dir":"Articles","previous_headings":"","what":"Visualizing group networks","title":"Model Comparison with bgmCompare","text":"can use output plot network ADHD group:","code":"library(qgraph)  adhd_network = matrix(0, 5, 5) adhd_network[lower.tri(adhd_network)] = coef(fit)$pairwise_effects_groups[, 1] adhd_network = adhd_network + t(adhd_network) colnames(adhd_network) = colnames(data_adhd) rownames(adhd_network) = colnames(data_adhd)  qgraph(adhd_network,   theme = \"TeamFortress\",   maximum = 1,   fade = FALSE,   color = c(\"#f0ae0e\"), vsize = 10, repulsion = .9,   label.cex = 1, label.scale = \"FALSE\",   labels = colnames(data_adhd) )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/comparison.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Model Comparison with bgmCompare","text":"one-sample analysis, see Getting Started vignette. diagnostics convergence checks, see Diagnostics vignette. additional analysis tools advanced plotting options, consider using easybgm package, integrates smoothly bgms objects.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Diagnostics and Spike-and-Slab Summaries","text":"vignette illustrates inspect convergence diagnostics interpret spike--slab summaries bgms models. model variables spike--slab priors introduce binary indicator variables govern whether effect included . posterior distributions can summarized inclusion probabilities Bayes factors.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"example-fit","dir":"Articles","previous_headings":"","what":"Example fit","title":"Diagnostics and Spike-and-Slab Summaries","text":"use subset Wenchuan dataset:","code":"library(bgms) data = Wenchuan[, 1:5] fit = bgm(data, seed = 1234)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"convergence-diagnostics","dir":"Articles","previous_headings":"","what":"Convergence diagnostics","title":"Diagnostics and Spike-and-Slab Summaries","text":"quality Markov chain can assessed common MCMC diagnostics: R-hat values close 1 (typically 1.01) suggest convergence (Vehtari et al., 2021). effective sample size (ESS) reflects number independent samples provide equivalent precision. Larger ESS values indicate reliable estimates. Monte Carlo standard error (MCSE) measures additional variability introduced using finite number MCMC draws. small MCSE relative posterior standard deviation indicates stable estimates, whereas large MCSE suggests samples needed. Advanced users can inspect traceplots extracting raw samples using external packages coda bayesplot. example using coda package create traceplot pairwise effect parameter.","code":"summary(fit)$pairwise #>                          mean          sd         mcse     n_eff #> intrusion-dreams  0.632293951 0.001661179 0.0657939558 1568.6981 #> intrusion-flash   0.335941200 0.001418260 0.0598391890 1780.1616 #> intrusion-upset   0.189497746 0.075619906 0.0054547304  192.1877 #> intrusion-physior 0.194802150 0.071131862 0.0049449384  206.9220 #> dreams-flash      0.502768076 0.001633580 0.0620233738 1441.5502 #> dreams-upset      0.228964481 0.060644348 0.0030175433  403.8998 #> dreams-physior    0.003975021 0.017652134 0.0006208483  808.3946 #> flash-upset       0.009480741 0.031081369 0.0013771486  509.3762 #> flash-physior     0.307821055 0.001453666 0.0560608285 1487.2689 #> upset-physior     0.710876259 0.001532448 0.0597194081 1518.6560 #>                       Rhat #> intrusion-dreams  1.003909 #> intrusion-flash   1.000981 #> intrusion-upset   1.005080 #> intrusion-physior 1.016768 #> dreams-flash      1.000174 #> dreams-upset      1.007224 #> dreams-physior    1.000268 #> flash-upset       1.009813 #> flash-physior     1.005348 #> upset-physior     1.000656 library(coda)  param_index = 1 chains = lapply(fit$raw_samples$pairwise, function(mat) mat[, param_index]) mcmc_obj = mcmc.list(lapply(chains, mcmc))  traceplot(mcmc_obj,   col = c(\"firebrick\", \"steelblue\", \"darkgreen\", \"goldenrod\"),   main = \"Traceplot of pairwise[1]\" )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"spike-and-slab-summaries","dir":"Articles","previous_headings":"","what":"Spike-and-slab summaries","title":"Diagnostics and Spike-and-Slab Summaries","text":"spike--slab prior yields posterior inclusion probabilities edges: Values near 1.0: strong evidence edge present. Values near 0.0: strong evidence edge absent. Values near 0.5: inconclusive (absence evidence).","code":"coef(fit)$indicator #>           intrusion  dreams   flash   upset physior #> intrusion   0.00000 1.00000 1.00000 0.92700 0.94875 #> dreams      1.00000 0.00000 1.00000 0.98475 0.04975 #> flash       1.00000 1.00000 0.00000 0.08775 1.00000 #> upset       0.92700 0.98475 0.08775 0.00000 1.00000 #> physior     0.94875 0.04975 1.00000 1.00000 0.00000"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"bayes-factors","dir":"Articles","previous_headings":"","what":"Bayes factors","title":"Diagnostics and Spike-and-Slab Summaries","text":"prior inclusion probability edge equal 0.5 (e.g., using Bernoulli prior inclusion_probability = 0.5 symmetric Beta prior, main_alpha = main_beta), can directly transform inclusion probabilities Bayes factors edge presence vs absence: Bayes factor favor inclusion (H1) small, meaning little evidence inclusion. Since Bayes factor transitive, can use express evidence favor exclusion (H0) Bayes factor shows strong evidence absence network relation variables intrusion physior.","code":"# Example for one edge p = coef(fit)$indicator[1, 5] BF_10 = p / (1 - p) BF_10 #> [1] 18.5122 1 / BF_10 #> [1] 0.05401845"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/diagnostics.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Diagnostics and Spike-and-Slab Summaries","text":"See Getting Started simple one-sample workflow. See Model Comparison group differences.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Getting Started with bgms","text":"bgms package implements Bayesian methods analyzing graphical models binary ordinal variables. estimates main effects (category thresholds) pairwise interactions ordinal Markov random field (MRF), optional Bayesian edge selection via spike––slab priors. package provides two main entry points: bgm() one-sample designs (single network), bgmCompare() independent-sample designs (group comparisons). vignette walks basic workflow: fitting model, summarizing posterior output, visualizing results.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"wenchuan-dataset","dir":"Articles","previous_headings":"","what":"Wenchuan dataset","title":"Getting Started with bgms","text":"dataset Wenchuan contains responses survivors 2008 Wenchuan earthquake posttraumatic stress items. , analyze subset first five items demonstration.","code":"library(bgms)  # Analyse a subset of the Wenchuan dataset ?Wenchuan data = Wenchuan[, 1:5] head(data) #>      intrusion dreams flash upset physior #> [1,]         2      2     2     2       3 #> [2,]         2      2     2     3       3 #> [3,]         2      4     4     4       3 #> [4,]         2      1     2     2       1 #> [5,]         2      2     2     2       2 #> [6,]         4      3     2     2       2"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"fitting-a-model","dir":"Articles","previous_headings":"","what":"Fitting a model","title":"Getting Started with bgms","text":"main entry point bgm() single-group models bgmCompare() multiple-group comparisons.","code":"fit = bgm(data, seed = 1234)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"posterior-summaries","dir":"Articles","previous_headings":"","what":"Posterior summaries","title":"Getting Started with bgms","text":"can also access posterior means inclusion probabilities directly:","code":"summary(fit) #> Posterior summaries from Bayesian estimation: #>  #> Category thresholds: #>                 mean  mcse    sd    n_eff  Rhat #> intrusion (1)  0.485 0.006 0.228 1588.251 1.006 #> intrusion (2) -1.883 0.010 0.331 1117.711 1.012 #> intrusion (3) -4.806 0.019 0.542  796.267 1.012 #> intrusion (4) -9.451 0.030 0.875  854.320 1.013 #> dreams (1)    -0.597 0.005 0.194 1404.284 1.004 #> dreams (2)    -3.802 0.011 0.351 1068.964 1.005 #> ... (use `summary(fit)$main` to see full output) #>  #> Pairwise interactions: #>                    mean    sd  mcse    n_eff  Rhat #> intrusion-dreams  0.632 0.002 0.066 1568.698 1.004 #> intrusion-flash   0.336 0.001 0.060 1780.162 1.001 #> intrusion-upset   0.189 0.076 0.005  192.188 1.005 #> intrusion-physior 0.195 0.071 0.005  206.922 1.017 #> dreams-flash      0.503 0.002 0.062 1441.550 1.000 #> dreams-upset      0.229 0.061 0.003  403.900 1.007 #> ... (use `summary(fit)$pairwise` to see full output) #> Note: NA values are suppressed in the print table. They occur here when an  #> indicator was zero across all iterations, so mcse/n_eff/Rhat are undefined; #> `summary(fit)$pairwise` still contains the NA values. #>  #> Inclusion probabilities: #>                    mean    sd  mcse n0->0 n0->1 n1->0 n1->1   n_eff #> intrusion-dreams  1.000 0.000           0     0     0  3999         #> intrusion-flash   1.000 0.000           0     0     0  3999         #> intrusion-upset   0.927 0.260 0.025   278    14    14  3693  106.19 #> intrusion-physior 0.949 0.221 0.023   196     9     9  3785  94.741 #> dreams-flash      1.000 0.000           0     0     0  3999         #> dreams-upset      0.985 0.123  0.01    57     4     4  3934 137.766 #>                    Rhat #> intrusion-dreams        #> intrusion-flash         #> intrusion-upset   1.052 #> intrusion-physior 1.123 #> dreams-flash            #> dreams-upset      1.162 #> ... (use `summary(fit)$indicator` to see full output) #> Note: NA values are suppressed in the print table. They occur when an indicator #> was constant (all 0 or all 1) across all iterations, so sd/mcse/n_eff/Rhat #> are undefined; `summary(fit)$indicator` still contains the NA values. #>  #> Use `summary(fit)$<component>` to access full results. #> See the `easybgm` package for other summary and plotting tools. coef(fit) #> $main #>              cat (1)   cat (2)   cat (3)    cat (4) #> intrusion  0.4848853 -1.883467 -4.806016  -9.451198 #> dreams    -0.5965478 -3.801521 -7.133627 -11.583398 #> flash     -0.1072322 -2.570423 -5.380047  -9.695791 #> upset      0.4224335 -1.297832 -3.366620  -7.029605 #> physior   -0.6101686 -3.158835 -6.201602 -10.539855 #>  #> $pairwise #>           intrusion      dreams       flash       upset     physior #> intrusion 0.0000000 0.632293951 0.335941200 0.189497746 0.194802150 #> dreams    0.6322940 0.000000000 0.502768076 0.228964481 0.003975021 #> flash     0.3359412 0.502768076 0.000000000 0.009480741 0.307821055 #> upset     0.1894977 0.228964481 0.009480741 0.000000000 0.710876259 #> physior   0.1948022 0.003975021 0.307821055 0.710876259 0.000000000 #>  #> $indicator #>           intrusion  dreams   flash   upset physior #> intrusion   0.00000 1.00000 1.00000 0.92700 0.94875 #> dreams      1.00000 0.00000 1.00000 0.98475 0.04975 #> flash       1.00000 1.00000 0.00000 0.08775 1.00000 #> upset       0.92700 0.98475 0.08775 0.00000 1.00000 #> physior     0.94875 0.04975 1.00000 1.00000 0.00000"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"network-plot","dir":"Articles","previous_headings":"","what":"Network plot","title":"Getting Started with bgms","text":"visualize network structure, threshold posterior inclusion probabilities 0.5 plot resulting adjacency matrix.","code":"library(qgraph)  median_probability_network = coef(fit)$pairwise median_probability_network[coef(fit)$indicator < 0.5] = 0.0  qgraph(median_probability_network,   theme = \"TeamFortress\",   maximum = 1,   fade = FALSE,   color = c(\"#f0ae0e\"), vsize = 10, repulsion = .9,   label.cex = 1, label.scale = \"FALSE\",   labels = colnames(data) )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/articles/intro.html","id":"next-steps","dir":"Articles","previous_headings":"","what":"Next steps","title":"Getting Started with bgms","text":"comparing groups, see ?bgmCompare Model Comparison vignette. diagnostics convergence checks, see Diagnostics vignette.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Maarten Marsman. Author, maintainer. Don van den Bergh. Author. Nikola Sekulovski. Contributor. Giuseppe Arena. Contributor. Laura Groot. Contributor. Gali Geller. Contributor.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Marsman, M., & van den Bergh, D. (2026). bgms: Bayesian Analysis Networks Binary /Ordinal Variables. R package version 0.1.6.3. Marsman M, van den Bergh D, Haslbeck J (2025). “Bayesian analysis ordinal Markov random field.” Psychometrika, 90(1), 146–182. doi:10.1017/psy.2024.4. Marsman M, Waldorp L, Sekulovski N, Haslbeck J (2025). “Bayes factor tests group differences ordinal binary graphical models.” Psychometrika, 90(5), 1809–1842. doi:10.1017/psy.2025.10060. Sekulovski N, Arena G, Haslbeck J, Huth K, Friel N, Marsman M (2025). “stochastic block prior clustering graphical models.” PsyArXiv Preprints, https://osf.io/preprints/psyarxiv/29p3m_v1.","code":"@Manual{bgms-package,   title = {bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables},   author = {Maarten Marsman and Don {van den Bergh}},   year = {2026},   note = {R package version 0.1.6.3},   url = {https://CRAN.R-project.org/package=bgms}, } @Article{Marsman2025-ordinal,   title = {Bayesian analysis of the ordinal Markov random field},   author = {Maarten Marsman and Don {van den Bergh} and Jolanda M. B. Haslbeck},   journal = {Psychometrika},   year = {2025},   volume = {90},   number = {1},   pages = {146--182},   doi = {10.1017/psy.2024.4}, } @Article{Marsman2025-bayes,   title = {Bayes factor tests for group differences in ordinal and binary graphical models},   author = {Maarten Marsman and Lourens J. Waldorp and Nikola Sekulovski and Jolanda M. B. Haslbeck},   journal = {Psychometrika},   year = {2025},   volume = {90},   number = {5},   pages = {1809--1842},   doi = {10.1017/psy.2025.10060}, } @Misc{Sekulovski2025-stochastic,   title = {A stochastic block prior for clustering in graphical models},   author = {Nikola Sekulovski and Giuseppe Arena and Jolanda M. B. Haslbeck and Karoline B. S. Huth and Nial Friel and Maarten Marsman},   year = {2025},   note = {PsyArXiv Preprints},   url = {https://osf.io/preprints/psyarxiv/29p3m_v1}, }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"main-functions","dir":"","previous_headings":"","what":"Main functions","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"package two main entry points: bgm() – estimates single network one-sample design. bgmCompare() – compares networks groups independent-sample design.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"effect-selection","dir":"","previous_headings":"","what":"Effect selection","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"functions support effect selection spike--slab priors: Edges one-sample designs: bgm() models presence absence edges variables. Posterior inclusion probabilities indicate plausibility edge can converted Bayes factors conditional independence tests (see Marsman, van den Bergh, et al., 2025; Sekulovski et al., 2024). Communities/clusters one-sample designs: bgm() can also model community structure. Posterior probabilities number clusters quantify plausibility clustering solutions can converted Bayes factors (see Sekulovski et al., 2025). Group differences independent-sample designs: bgmCompare() models differences edge weights category thresholds groups. Posterior inclusion probabilities indicate plausibility parameter differences can converted Bayes factors tests parameter equivalence (see Marsman, Waldorp, et al., 2025).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"learn-more","dir":"","previous_headings":"","what":"Learn more","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"worked examples tutorials, see package vignettes: Getting Started Model Comparison Diagnostics Spike--Slab Summaries can also access directly R :","code":"browseVignettes(\"bgms\")"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"why-use-markov-random-fields","dir":"","previous_headings":"","what":"Why use Markov Random Fields?","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"Graphical models networks become central recent psychological psychometric research (Contreras et al., 2019; Marsman & Rhemtulla, 2022; Robinaugh et al., 2020). Markov random field (MRF) models, graph structure reflects partial associations variables (Kindermann & Snell, 1980). MRF, missing edge two variables implies conditional independence given rest network (Lauritzen, 2004). words, remaining variables fully explain away potential association unconnected pair.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"why-use-a-bayesian-approach","dir":"","previous_headings":"","what":"Why use a Bayesian approach?","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"analyzing MRF, often want compare competing hypotheses: Edge presence vs. edge absence (conditional dependence vs. independence) one-sample designs. Parameter difference vs. parameter equivalence independent-sample designs. Frequentist approaches limited comparisons: can reject null hypothesis, provide evidence . result, edge difference excluded, remains unclear whether reflects true absence simply insufficient power. Bayesian inference avoids problem. Using inclusion Bayes factors (Huth et al., 2023; Sekulovski et al., 2024), can quantify evidence directions: Evidence edge presence vs. evidence edge absence, Evidence parameter difference vs. evidence parameter equivalence. makes possible detect structure group differences, also conclude absence evidence.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Bayesian Analysis of Networks of Binary and/or Ordinal Variables","text":"current developmental version can installed ","code":"if(!requireNamespace(\"remotes\")) {   install.packages(\"remotes\") } remotes::install_github(\"Bayesian-Graphical-Modelling-Lab/bgms\")"},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":null,"dir":"Reference","previous_headings":"","what":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"dataset includes ADHD symptom ratings 355 children aged 6 8 years Children’s Attention Project (CAP) cohort (Silk et al. 2019) . sample consists 146 children diagnosed ADHD 209 without diagnosis. Symptoms assessed structured interviews parents using NIMH Diagnostic Interview Schedule Children IV (DISC-IV) (Shaffer et al. 2000) . checklist includes 18 items: 9 Inattentive () 9 Hyperactive/Impulsive (HI). item binary (1 = present, 0 = absent).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"","code":"data(\"ADHD\")"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"matrix 355 rows 19 columns. group ADHD diagnosis: 1 = diagnosed, 0 = diagnosed avoid Often avoids, dislikes, reluctant engage tasks   require sustained mental effort () closeatt Often fails give close attention details makes   careless mistakes schoolwork, work, activities () distract often easily distracted extraneous stimuli () forget often forgetful daily activities () instruct Often follow instructions fails   finish schoolwork, chores, duties workplace () listen Often seem listen spoken directly   () loses Often loses things necessary tasks activities () org Often difficulty organizing tasks activities () susatt Often difficulty sustaining attention tasks play   activities () blurts Often blurts answers questions completed   (HI) fidget Often fidgets hands feet squirms seat   (HI) interrupt Often interrupts intrudes others (HI) motor often \"go\" often acts \"driven motor\"   (HI) quiet Often difficulty playing engaging leisure activities   quietly (HI) runs Often runs climbs excessively situations   inappropriate (HI) seat Often leaves seat classroom situations   remaining seated expected (HI) talks Often talks excessively (HI) turn Often difficulty awaiting turn (HI)","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"Silk et al. (2019) . Data retrieved doi:10.1371/journal.pone.0211053.s004 . Licensed CC-4.0: https://creativecommons.org/licenses//4.0/","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/ADHD.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"ADHD Symptom Checklist for Children Aged 6–8 Years — ADHD","text":"Shaffer D, Fisher P, Lucas CP, Dulcan MK, Schwab-Stone (2000). “NIMH Diagnostic Interview Schedule Children Version IV (NIMH DISC-IV): description, differences previous versions, reliability common diagnoses.” Journal American Academy Child & Adolescent Psychiatry, 39, 28–38. doi:10.1097/00004583-200001000-00014 , PMID: 10638065. Silk TJ, Malpas CB, Beare R, Efron D, Anderson V, Hazell P, Jongeling B, Nicholson JM, Sciberras E (2019). “network analysis approach ADHD symptoms: sum parts.” PLOS ONE, 14(1), e0211053. doi:10.1371/journal.pone.0211053 .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":null,"dir":"Reference","previous_headings":"","what":"Short Boredom Proneness Scale Responses — Boredom","title":"Short Boredom Proneness Scale Responses — Boredom","text":"dataset includes responses 8-item Short Boredom Proneness Scale (SBPS), self-report measure individual's susceptibility boredom (Martarelli et al. 2023) . Items rated 7-point Likert scale ranging 1 (\"strongly disagree\") 7 (\"strongly agree\"). scale administered either English (Struk et al. 2015)  French (translated (Martarelli et al. 2023) ).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Short Boredom Proneness Scale Responses — Boredom","text":"","code":"data(\"Boredom\")"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"Short Boredom Proneness Scale Responses — Boredom","text":"matrix 986 rows 9 columns. row corresponds respondent. language Language SBPS administered: \"en\" = English, \"fr\" = French loose_ends often find “loose ends,” knowing   . entertain find hard entertain . repetitive Many things repetitive monotonous. stimulation takes stimulation get going   people. motivated feel motivated things . keep_interest situations, hard find   something see keep interested. sit_around Much time, just sit around nothing. half_dead_dull Unless something exciting, even dangerous,   feel half-dead dull.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"Short Boredom Proneness Scale Responses — Boredom","text":"Martarelli et al. (2023) . Data retrieved https://osf.io/qhux8. Licensed CC-4.0: https://creativecommons.org/licenses//4.0/","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Boredom.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Short Boredom Proneness Scale Responses — Boredom","text":"Martarelli CS, Baillifard , Audrin C (2023). “Trait-Based Network Perspective Validation French Short Boredom Proneness Scale.” European Journal Psychological Assessment, 39(6), 390–399. doi:10.1027/1015-5759/a000718 . Struk AA, Carriere JSA, Cheyne JA, Danckert J (2015). “Short Boredom Proneness Scale: Development Psychometric Properties.” Assessment, 24(3), 346–359. doi:10.1177/1073191115609996 .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":null,"dir":"Reference","previous_headings":"","what":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"dataset contains responses 17 items assessing symptoms post-traumatic stress disorder (PTSD) Chinese adults survived 2008 Wenchuan earthquake lost least one child disaster (McNally et al. 2015) . Participants completed civilian version Posttraumatic Checklist, item corresponding DSM-IV PTSD symptom. Items rated 5-point Likert scale \"\" \"extremely,\" indicating degree symptom bothered respondent past month.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"","code":"data(\"Wenchuan\")"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":"format","dir":"Reference","previous_headings":"","what":"Format","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"matrix 362 rows 17 columns. row represents participant. intrusion Repeated, disturbing memories, thoughts, images   stressful experience past? dreams Repeated, disturbing dreams stressful experience   past? flash Suddenly acting feeling stressful experience   happening (reliving )? upset Feeling upset something reminded stressful   experience past? physior physical reactions (e.g., heart pounding, trouble   breathing, sweating) something reminded stressful experience   past? avoidth Avoiding thinking talking stressful   experience past avoiding feelings related ? avoidact Avoiding activities situations reminded   stressful experience past? amnesia Trouble remembering important parts stressful   experience past? lossint Loss interest activities used enjoy? distant Feeling distant cut people? numb Feeling emotionally numb unable loving   feelings close ? future Feeling future somehow cut short? sleep Trouble falling staying asleep? anger Feeling irritable angry outbursts? concen difficulty concentrating? hyper \"super-alert\" watchful guard? startle Feeling jumpy easily startled?","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":"source","dir":"Reference","previous_headings":"","what":"Source","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"https://psychosystems.org/wp-content/uploads/2014/10/Wenchuan.csv","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/Wenchuan.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"PTSD Symptoms in Wenchuan Earthquake Survivors Who Lost a Child — Wenchuan","text":"McNally RJ, Robinaugh DJ, Wu GWY, Wang L, Deserno MK, Borsboom D (2015). “Mental disorders causal systems: network approach posttraumatic stress disorder.” Clinical Psychological Science, 6, 836–849. doi:10.1177/2167702614553230 .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"bgm function estimates pseudoposterior distribution category thresholds (main effects) pairwise interaction parameters Markov Random Field (MRF) model binary /ordinal variables. Optionally, performs Bayesian edge selection using spike--slab priors infer network structure.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"","code":"bgm(   x,   variable_type = \"ordinal\",   baseline_category,   iter = 1000,   warmup = 1000,   pairwise_scale = 2.5,   main_alpha = 0.5,   main_beta = 0.5,   edge_selection = TRUE,   edge_prior = c(\"Bernoulli\", \"Beta-Bernoulli\", \"Stochastic-Block\"),   inclusion_probability = 0.5,   beta_bernoulli_alpha = 1,   beta_bernoulli_beta = 1,   beta_bernoulli_alpha_between = 1,   beta_bernoulli_beta_between = 1,   dirichlet_alpha = 1,   lambda = 1,   na_action = c(\"listwise\", \"impute\"),   update_method = c(\"nuts\", \"adaptive-metropolis\", \"hamiltonian-mc\"),   target_accept,   hmc_num_leapfrogs = 100,   nuts_max_depth = 10,   learn_mass_matrix = TRUE,   chains = 4,   cores = parallel::detectCores(),   display_progress = c(\"per-chain\", \"total\", \"none\"),   seed = NULL,   standardize = FALSE,   verbose = getOption(\"bgms.verbose\", TRUE),   interaction_scale,   burnin,   save,   threshold_alpha,   threshold_beta )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"x data frame matrix n rows p columns containing binary ordinal responses. Variables automatically recoded non-negative integers (0, 1, ..., m). regular ordinal variables, unobserved categories collapsed; Blume–Capel variables, categories retained. variable_type Character character vector. Specifies type variable x. Allowed values: \"ordinal\" \"blume-capel\". Binary variables automatically treated \"ordinal\". Default: \"ordinal\". baseline_category Integer vector. Baseline category used Blume–Capel variables. Can single integer (applied ) vector length p. Required least one variable type \"blume-capel\". iter Integer. Number post–burn-iterations (per chain). Default: 1e3. warmup Integer. Number warmup iterations collecting samples. minimum 1000 iterations enforced, warning smaller value requested. Default: 1e3. pairwise_scale Double. Scale Cauchy prior pairwise interaction parameters. Default: 2.5. main_alpha, main_beta Double. Shape parameters beta-prime prior threshold parameters. Must positive. equal, prior symmetric. Defaults: main_alpha = 0.5 main_beta = 0.5. edge_selection Logical. Whether perform Bayesian edge selection. FALSE, model estimates edges. Default: TRUE. edge_prior Character. Specifies prior edge inclusion. Options: \"Bernoulli\", \"Beta-Bernoulli\", \"Stochastic-Block\". Default: \"Bernoulli\". inclusion_probability Numeric scalar. Prior inclusion probability edge (used Bernoulli prior). Default: 0.5. beta_bernoulli_alpha, beta_bernoulli_beta Double. Shape parameters beta distribution Beta–Bernoulli Stochastic-Block priors. Must positive. Stochastic-Block prior shape parameters within-cluster edge inclusion probabilities. Defaults: beta_bernoulli_alpha = 1 beta_bernoulli_beta = 1. beta_bernoulli_alpha_between, beta_bernoulli_beta_between Double. Shape parameters -cluster edge inclusion probabilities Stochastic-Block prior. Must positive. Default: beta_bernoulli_alpha_between = 1 beta_bernoulli_beta_between = 1 dirichlet_alpha Double. Concentration parameter Dirichlet prior block assignments (used Stochastic Block model). Default: 1. lambda Double. Rate zero-truncated Poisson prior number clusters Stochastic Block Model. Default: 1. na_action Character. Specifies missing data handling. Either \"listwise\" (drop rows missing values) \"impute\" (perform single imputation sampling). Default: \"listwise\". update_method Character. Specifies MCMC sampler updates model parameters: \"adaptive-metropolis\" Componentwise adaptive Metropolis–Hastings     Robbins–Monro proposal adaptation. \"hamiltonian-mc\" Hamiltonian Monte Carlo fixed path length     (number leapfrog steps set hmc_num_leapfrogs). \"nuts\" -U-Turn Sampler, adaptive form HMC     dynamically chosen trajectory lengths. Default: \"nuts\". target_accept Numeric 0 1. Target acceptance rate sampler. Defaults set automatically supplied: 0.44 adaptive Metropolis, 0.65 HMC, 0.80 NUTS. hmc_num_leapfrogs Integer. Number leapfrog steps Hamiltonian Monte Carlo. Must positive. Default: 100. nuts_max_depth Integer. Maximum tree depth NUTS. Must positive. Default: 10. learn_mass_matrix Logical. TRUE, adapt diagonal mass matrix warmup (HMC/NUTS ). FALSE, use identity matrix. Default: TRUE. chains Integer. Number parallel chains run. Default: 4. cores Integer. Number CPU cores parallel execution. Default: parallel::detectCores(). display_progress Character. Controls progress reporting sampling. Options: \"per-chain\" (separate bar per chain), \"total\" (single combined bar), \"none\" (progress). Default: \"per-chain\". seed Optional integer. Random seed reproducibility. Must single non-negative integer. standardize Logical. TRUE, Cauchy prior scale pairwise interaction adjusted based range response scores. Variables response categories larger score products \\(x_i \\cdot x_j\\), typically correspond smaller interaction effects \\(\\sigma_{ij}\\). Without standardization, fixed prior scale relatively wide smaller effects, resulting less shrinkage high-category pairs shrinkage low-category pairs. Standardization scales prior proportionally maximum score product, ensuring equivalent relative shrinkage across pairs. internal recoding, regular ordinal variables scores \\(0, 1, \\ldots, m\\). adjusted scale interaction variables \\(\\) \\(j\\) pairwise_scale * m_i * m_j, pairwise_scale applies unit interval case (binary variables \\(m_i = m_j = 1\\)). Blume-Capel variables reference category \\(b\\), scores centered \\(-b, \\ldots, m-b\\), adjustment uses maximum absolute product score endpoints. mixed pairs, ordinal variables use raw score endpoints \\((0, m)\\) Blume-Capel variables use centered score endpoints \\((-b, m-b)\\). Default: FALSE. verbose Logical. TRUE, prints informational messages data processing (e.g., missing data handling, variable recoding). Defaults getOption(\"bgms.verbose\", TRUE). Set options(bgms.verbose = FALSE) suppress messages globally. interaction_scale, burnin, save, threshold_alpha, threshold_beta `r lifecycle::badge(\"deprecated\")` Deprecated arguments **bgms 0.1.6.0**. Use `pairwise_scale`, `warmup`, `main_alpha`, `main_beta` instead.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"list class \"bgms\" posterior summaries, posterior mean matrices, access raw MCMC draws. object can passed print(), summary(), coef(). Main components include: posterior_summary_main: Data frame posterior summaries     (mean, sd, MCSE, ESS, Rhat) category threshold parameters. posterior_summary_pairwise: Data frame posterior     summaries pairwise interaction parameters. posterior_summary_indicator: Data frame posterior     summaries edge inclusion indicators (edge_selection = TRUE). posterior_mean_main: Matrix posterior mean thresholds     (rows = variables, cols = categories parameters). posterior_mean_pairwise: Symmetric matrix posterior mean     pairwise interaction strengths. posterior_mean_indicator: Symmetric matrix posterior mean     inclusion probabilities (edge selection enabled). Additional summaries returned     edge_prior = \"Stochastic-Block\". details prior     see Sekulovski et al. (2025) . posterior_summary_pairwise_allocations: Data frame       posterior summaries (mean, sd, MCSE, ESS, Rhat) pairwise       cluster co-occurrence nodes. serves indicate       whether estimated posterior allocations,co-clustering matrix       posterior cluster probabilities (see blow) converged. posterior_coclustering_matrix: symmetric matrix       pairwise proportions occurrence every variable. matrix       can plotted visually inspect estimated number clusters       visually inspect nodes tend switch clusters. posterior_mean_allocations: vector posterior mean       cluster allocations nodes. calculated using method       proposed Dahl (2009) . posterior_mode_allocations: vector posterior        mode cluster allocations nodes. posterior_num_blocks: data frame estimated       posterior inclusion probabilities possible number clusters. raw_samples: list raw MCMC draws per chain: main List main effect samples. pairwise List pairwise effect samples. indicator List indicator samples         (edge selection enabled). allocations List cluster allocations         (SBM prior used). nchains Number chains. niter Number post–warmup iterations per chain. parameter_names Named lists parameter labels. arguments: list function call arguments metadata     (e.g., number variables, warmup, sampler settings, package version). summary() method prints formatted posterior summaries, coef() extracts posterior mean matrices. NUTS diagnostics (tree depth, divergences, energy, E-BFMI) included fit$nuts_diag update_method = \"nuts\".","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"function models joint distribution binary ordinal variables using Markov Random Field, support edge selection Bayesian variable selection. statistical foundation model described Marsman et al. (2025) , ordinal MRF model Bayesian estimation procedure first introduced. implementation bgms since extended updated (e.g., alternative priors, parallel chains, HMC/NUTS warmup), builds original framework. Key components model described sections .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"ordinal-variables","dir":"Reference","previous_headings":"","what":"Ordinal Variables","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"function supports two types ordinal variables: Regular ordinal variables: Assigns category threshold parameter response category except lowest. model imposes additional constraints distribution category responses. Blume-Capel ordinal variables: Assume baseline category (e.g., “neutral” response) score responses distance baseline. Category thresholds modeled : $$\\mu_{c} = \\alpha \\cdot (c-b) + \\beta \\cdot (c - b)^2$$ : \\(\\mu_{c}\\): category threshold category \\(c\\) \\(\\alpha\\): linear trend across categories \\(\\beta\\): preference toward away baseline \\(\\beta < 0\\), model favors responses near baseline      category; \\(\\beta > 0\\), favors responses farther away (.e.,      extremes). \\(b\\): baseline category Accordingly, pairwise interactions Blume-Capel variables modeled terms \\(c-b\\) scores.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"edge-selection","dir":"Reference","previous_headings":"","what":"Edge Selection","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"edge_selection = TRUE, function performs Bayesian variable selection pairwise interactions (edges) MRF using spike--slab priors. Supported priors edge inclusion: Bernoulli: Fixed inclusion probability across edges. Beta-Bernoulli: Inclusion probability assigned Beta   prior distribution. Stochastic-Block: Cluster-based edge priors Beta,   Dirichlet, Poisson hyperpriors. priors operate via binary indicator variables controlling inclusion exclusion edge MRF.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"prior-distributions","dir":"Reference","previous_headings":"","what":"Prior Distributions","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"Pairwise effects: Modeled Cauchy (slab) prior. Main effects: Modeled using beta-prime   distribution. Edge indicators: Use either Bernoulli, Beta-Bernoulli,   Stochastic-Block prior ().","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"sampling-algorithms-and-warmup","dir":"Reference","previous_headings":"","what":"Sampling Algorithms and Warmup","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"Parameters updated within Gibbs framework, conditional updates can carried using different algorithms: Adaptive Metropolis–Hastings: Componentwise random–walk     updates main effects pairwise effects. Proposal standard     deviations adapted burn–via Robbins–Monro updates     toward target acceptance rate. Hamiltonian Monte Carlo (HMC): Joint updates     parameters using fixed–length leapfrog trajectories. Step size     tuned warmup via dual–averaging; diagonal mass matrix can     also adapted learn_mass_matrix = TRUE. –U–Turn Sampler (NUTS): adaptive extension HMC     dynamically chooses trajectory lengths. Warmup uses staged     adaptation schedule (fast–slow–fast) stabilize step size ,     enabled, mass matrix. edge_selection = TRUE, updates edge–inclusion indicators carried Metropolis–Hastings steps. switched core warmup phase, ensuring graph updates occur samplers’ tuning parameters (step size, mass matrix, proposal SDs) stabilized. warmup, adaptation disabled. Step size mass matrix fixed learned values, proposal SDs remain constant.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"warmup-and-adaptation","dir":"Reference","previous_headings":"","what":"Warmup and Adaptation","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"warmup procedure bgm uses multi-stage adaptation schedule (Stan Development Team 2023) . Warmup iterations split several phases: Stage 1 (fast adaptation): short initial interval     step size (HMC/NUTS) adapted, allowing chain     move quickly toward typical set. Stage 2 (slow windows): sequence expanding,     memoryless windows step size ,     learn_mass_matrix = TRUE, diagonal mass matrix     adapted. window ends reset dual–averaging scheme     improved stability. Stage 3a (final fast interval): short interval     end core warmup step size adapted one final time. Stage 3b (proposal–SD tuning): active     edge_selection = TRUE HMC/NUTS. phase,     Robbins–Monro adaptation proposal standard deviations     performed Metropolis steps used edge–selection moves. Stage 3c (graph selection warmup): Also relevant     edge_selection = TRUE. start phase,     random graph structure initialized, Metropolis–Hastings     updates edge inclusion indicators switched . edge_selection = FALSE, total number warmup iterations equals user–specified burnin. edge_selection = TRUE update_method \"nuts\" \"hamiltonian-mc\", schedule automatically appends additional Stage-3b Stage-3c intervals, total warmup strictly greater requested burnin. warmup phases, sampler transitions sampling phase adaptation disabled. Step size mass matrix (HMC/NUTS) fixed learned values, proposal SDs remain constant. staged design improves stability proposals ensures local parameters (step size) global parameters (mass matrix, proposal SDs) tuned collecting posterior samples. adaptive Metropolis–Hastings runs, step size mass matrix adaptation relevant. Proposal SDs tuned continuously burn–using Robbins–Monro updates, without staged fast/slow intervals.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"missing-data","dir":"Reference","previous_headings":"","what":"Missing Data","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"na_action = \"listwise\", observations missing values removed. na_action = \"impute\", missing values imputed Gibbs sampling.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"Dahl DB (2009). “Modal clustering class product partition models.” Bayesian Analysis, 4(2), 243–264. doi:10.1214/09-BA409 . Marsman M, van den Bergh D, Haslbeck JMB (2025). “Bayesian analysis ordinal Markov random field.” Psychometrika, 90(1), 146–182. doi:10.1017/psy.2024.4 . Sekulovski N, Arena G, Haslbeck JMB, Huth KBS, Friel N, Marsman M (2025). “Stochastic Block Prior Clustering Graphical Models.” Retrieved https://osf.io/preprints/psyarxiv/29p3m_v1. OSF preprint. Stan Development Team (2023). Stan Modeling Language Users Guide Reference Manual. Version 2.33, https://mc-stan.org/docs/.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgm.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Estimation or Edge Selection for Markov Random Fields — bgm","text":"","code":"# \\donttest{ # Run bgm on subset of the Wenchuan dataset fit = bgm(x = Wenchuan[, 1:5]) #> 7 rows with missing values excluded (n = 355 remaining). #> To impute missing values instead, use na_action = \"impute\". #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 50/2000 (2.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 50/2000 (2.5%) #> Chain 3 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 60/2000 (3.0%) #> Chain 4 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 61/2000 (3.0%) #> Total   (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 221/8000 (2.8%) #> Elapsed: 2s | ETA: 1m 10s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2000 (5.0%) #> Chain 2 (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 158/2000 (7.9%) #> Chain 3 (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 169/2000 (8.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 192/2000 (9.6%) #> Total   (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 619/8000 (7.7%) #> Elapsed: 3s | ETA: 36s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 200/2000 (10.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 278/2000 (13.9%) #> Chain 3 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 308/2000 (15.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 334/2000 (16.7%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1120/8000 (14.0%) #> Elapsed: 4s | ETA: 25s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 300/2000 (15.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 393/2000 (19.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 448/2000 (22.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 454/2000 (22.7%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1595/8000 (19.9%) #> Elapsed: 5s | ETA: 20s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 400/2000 (20.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 505/2000 (25.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 547/2000 (27.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 576/2000 (28.8%) #> Total   (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2028/8000 (25.4%) #> Elapsed: 5s | ETA: 15s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 550/2000 (27.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 680/2000 (34.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 742/2000 (37.1%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 750/2000 (37.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2722/8000 (34.0%) #> Elapsed: 6s | ETA: 12s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 700/2000 (35.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 832/2000 (41.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 888/2000 (44.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 886/2000 (44.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3306/8000 (41.3%) #> Elapsed: 7s | ETA: 10s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 850/2000 (42.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 942/2000 (47.1%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1012/2000 (50.6%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1021/2000 (51.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 3825/8000 (47.8%) #> Elapsed: 8s | ETA: 9s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 950/2000 (47.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1067/2000 (53.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1137/2000 (56.9%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1134/2000 (56.7%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 4288/8000 (53.6%) #> Elapsed: 8s | ETA: 7s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1100/2000 (55.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1210/2000 (60.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1288/2000 (64.4%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1260/2000 (63.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 4858/8000 (60.7%) #> Elapsed: 9s | ETA: 6s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1250/2000 (62.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1326/2000 (66.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1429/2000 (71.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1389/2000 (69.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5394/8000 (67.4%) #> Elapsed: 10s | ETA: 5s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1400/2000 (70.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1480/2000 (74.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1577/2000 (78.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1538/2000 (76.9%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5995/8000 (74.9%) #> Elapsed: 10s | ETA: 3s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1550/2000 (77.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 1625/2000 (81.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1726/2000 (86.3%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1675/2000 (83.8%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6576/8000 (82.2%) #> Elapsed: 11s | ETA: 2s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1650/2000 (82.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1718/2000 (85.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1833/2000 (91.6%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1777/2000 (88.8%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6978/8000 (87.2%) #> Elapsed: 12s | ETA: 2s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1750/2000 (87.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 1817/2000 (90.8%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1947/2000 (97.4%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1881/2000 (94.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7394/8000 (92.4%) #> Elapsed: 12s | ETA: 1s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1900/2000 (95.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 1917/2000 (95.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 1974/2000 (98.7%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7791/8000 (97.4%) #> Elapsed: 13s | ETA: 0s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8000/8000 (100.0%) #> Elapsed: 13s | ETA: 0s  # Posterior inclusion probabilities summary(fit)$indicator #>                      mean         sd        mcse n0->0 n0->1 n1->0 #> intrusion-dreams  1.00000 0.00000000          NA     0     0     0 #> intrusion-flash   1.00000 0.00000000          NA     0     0     0 #> intrusion-upset   0.95350 0.21056531 0.020634018   177     9     9 #> intrusion-physior 0.95300 0.21163884 0.018803391   177    11    11 #> dreams-flash      1.00000 0.00000000          NA     0     0     0 #> dreams-upset      0.99875 0.03533324 0.001674838     4     1     1 #> dreams-physior    0.06350 0.24386010 0.009061116  3673    72    73 #> flash-upset       0.06900 0.25345414 0.011206714  3665    59    58 #> flash-physior     1.00000 0.00000000          NA     0     0     0 #> upset-physior     1.00000 0.00000000          NA     0     0     0 #>                   n1->1    n_eff     Rhat #> intrusion-dreams   3999       NA       NA #> intrusion-flash    3999       NA       NA #> intrusion-upset    3804 104.1372 1.157488 #> intrusion-physior  3800 126.6830 1.055861 #> dreams-flash       3999       NA       NA #> dreams-upset       3993 445.0627 1.293320 #> dreams-physior      181 724.2994 1.015570 #> flash-upset         217 511.4960 1.068617 #> flash-physior      3999       NA       NA #> upset-physior      3999       NA       NA  # Posterior pairwise effects summary(fit)$pairwise #>                          mean          sd         mcse     n_eff #> intrusion-dreams  0.631255940 0.001554207 0.0667500309 1844.5267 #> intrusion-flash   0.337831281 0.001385307 0.0618520705 1993.4996 #> intrusion-upset   0.196914526 0.070992831 0.0045237563  246.2808 #> intrusion-physior 0.195512032 0.069395933 0.0041081643  285.3464 #> dreams-flash      0.501475564 0.001256171 0.0586106935 2176.9877 #> dreams-upset      0.229125793 0.053452640 0.0015553903 1181.0262 #> dreams-physior    0.005979916 0.023247067 0.0009544215  593.2748 #> flash-upset       0.007233699 0.026876728 0.0012391145  470.4678 #> flash-physior     0.308794047 0.001328681 0.0543685742 1674.3827 #> upset-physior     0.707411696 0.001505512 0.0591729274 1544.8201 #>                        Rhat #> intrusion-dreams  1.0001602 #> intrusion-flash   1.0007993 #> intrusion-upset   1.0217084 #> intrusion-physior 1.0109454 #> dreams-flash      0.9999511 #> dreams-upset      1.0042391 #> dreams-physior    1.0066340 #> flash-upset       1.0169991 #> flash-physior     1.0061400 #> upset-physior     1.0018740 # }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"bgmCompare function estimates group differences category threshold parameters (main effects) pairwise interactions (pairwise effects) Markov Random Field (MRF) binary ordinal variables. Groups can defined either supplying two separate datasets (x y) group membership vector. Optionally, Bayesian variable selection can applied identify differences across groups.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"","code":"bgmCompare(   x,   y,   group_indicator,   difference_selection = TRUE,   main_difference_selection = FALSE,   variable_type = \"ordinal\",   baseline_category,   difference_scale = 1,   difference_prior = c(\"Bernoulli\", \"Beta-Bernoulli\"),   difference_probability = 0.5,   beta_bernoulli_alpha = 1,   beta_bernoulli_beta = 1,   pairwise_scale = 2.5,   main_alpha = 0.5,   main_beta = 0.5,   iter = 1000,   warmup = 1000,   na_action = c(\"listwise\", \"impute\"),   update_method = c(\"nuts\", \"adaptive-metropolis\", \"hamiltonian-mc\"),   target_accept,   hmc_num_leapfrogs = 100,   nuts_max_depth = 10,   learn_mass_matrix = TRUE,   chains = 4,   cores = parallel::detectCores(),   display_progress = c(\"per-chain\", \"total\", \"none\"),   seed = NULL,   standardize = FALSE,   verbose = getOption(\"bgms.verbose\", TRUE),   main_difference_model,   reference_category,   main_difference_scale,   pairwise_difference_scale,   pairwise_difference_prior,   main_difference_prior,   pairwise_difference_probability,   main_difference_probability,   pairwise_beta_bernoulli_alpha,   pairwise_beta_bernoulli_beta,   main_beta_bernoulli_alpha,   main_beta_bernoulli_beta,   interaction_scale,   threshold_alpha,   threshold_beta,   burnin,   save )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"x data frame matrix binary ordinal responses Group 1. Variables coded nonnegative integers starting 0. ordinal variables, unused categories collapsed; Blume–Capel variables, categories retained. y Optional data frame matrix Group 2 (two-group designs). Must variables (columns) x. group_indicator Optional integer vector group memberships rows x (multi-group designs). Ignored y supplied. difference_selection Logical. TRUE, spike--slab priors applied difference parameters. Default: TRUE. main_difference_selection Logical. TRUE, apply spike--slab selection main effect (threshold) differences. FALSE, main effect differences always included (selection). Since main effects often nuisance parameters selection can interfere pairwise selection Beta-Bernoulli prior, default FALSE. used difference_selection = TRUE. variable_type Character vector specifying type variable: \"ordinal\" (default) \"blume-capel\". baseline_category Integer vector giving baseline category Blume–Capel variables. difference_scale Double. Scale Cauchy prior difference parameters. Default: 1. difference_prior Character. Prior difference inclusion: \"Bernoulli\" \"Beta-Bernoulli\". Default: \"Bernoulli\". difference_probability Numeric. Prior inclusion probability differences (Bernoulli prior). Default: 0.5. beta_bernoulli_alpha, beta_bernoulli_beta Doubles. Shape parameters Beta prior inclusion probabilities Beta–Bernoulli model. Defaults: 1. pairwise_scale Double. Scale Cauchy prior baseline pairwise interactions. Default: 2.5. main_alpha, main_beta Doubles. Shape parameters beta-prime prior baseline threshold parameters. Defaults: 0.5. iter Integer. Number post–warmup iterations per chain. Default: 1e3. warmup Integer. Number warmup iterations sampling. Default: 1e3. na_action Character. handle missing data: \"listwise\" (drop rows) \"impute\" (impute within Gibbs). Default: \"listwise\". update_method Character. Sampling algorithm: \"adaptive-metropolis\", \"hamiltonian-mc\", \"nuts\". Default: \"nuts\". target_accept Numeric 0 1. Target acceptance rate. Defaults: 0.44 (Metropolis), 0.65 (HMC), 0.80 (NUTS). hmc_num_leapfrogs Integer. Leapfrog steps HMC. Default: 100. nuts_max_depth Integer. Maximum tree depth NUTS. Default: 10. learn_mass_matrix Logical. TRUE, adapts diagonal mass matrix warmup (HMC/NUTS ). Default: TRUE. chains Integer. Number parallel chains. Default: 4. cores Integer. Number CPU cores. Default: parallel::detectCores(). display_progress Character. Controls progress reporting: \"per-chain\", \"total\", \"none\". Default: \"per-chain\". seed Optional integer. Random seed reproducibility. standardize Logical. TRUE, Cauchy prior scale pairwise interaction (baseline difference) adjusted based range response scores. Without standardization, pairs response categories experience less shrinkage naturally smaller interaction effects make fixed prior relatively wide. Standardization equalizes relative shrinkage across pairs, pairwise_scale applying unit interval (binary) case. See bgm details adjustment. Default: FALSE. verbose Logical. TRUE, prints informational messages data processing (e.g., missing data handling, variable recoding). Defaults getOption(\"bgms.verbose\", TRUE). Set options(bgms.verbose = FALSE) suppress messages globally. main_difference_model, reference_category, pairwise_difference_scale, main_difference_scale, pairwise_difference_prior, main_difference_prior, pairwise_difference_probability, main_difference_probability, pairwise_beta_bernoulli_alpha, pairwise_beta_bernoulli_beta, main_beta_bernoulli_alpha, main_beta_bernoulli_beta, interaction_scale, threshold_alpha, threshold_beta, burnin, save `r lifecycle::badge(\"deprecated\")` Deprecated arguments **bgms 0.1.6.0**. Use `difference_scale`, `difference_prior`, `difference_probability`, `beta_bernoulli_alpha`, `beta_bernoulli_beta`, `baseline_category`, `pairwise_scale`, `warmup` instead.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"list class \"bgmCompare\" containing posterior summaries, posterior mean matrices, raw MCMC samples: posterior_summary_main_baseline,     posterior_summary_pairwise_baseline: summaries baseline     thresholds pairwise interactions. posterior_summary_main_differences,     posterior_summary_pairwise_differences: summaries group     differences thresholds pairwise interactions. posterior_summary_indicator: summaries inclusion     indicators (difference_selection = TRUE). posterior_mean_main_baseline,     posterior_mean_pairwise_baseline: posterior mean matrices     (legacy style). raw_samples: list raw draws per chain main,     pairwise, indicator parameters. arguments: list function call arguments metadata. summary() method prints formatted summaries, coef() extracts posterior means. NUTS diagnostics (tree depth, divergences, energy, E-BFMI) included fit$nuts_diag update_method = \"nuts\".","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"function extends ordinal MRF framework Marsman et al. (2025)  multiple groups. basic idea modeling, analyzing, testing group differences MRFs introduced Marsman et al. (2025) , two–group comparisons conducted using adaptive Metropolis sampling. present implementation generalizes approach two groups supports additional samplers (HMC NUTS) staged warmup adaptation. Key components model:","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"pairwise-interactions","dir":"Reference","previous_headings":"","what":"Pairwise Interactions","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"variables \\(\\) \\(j\\), group-specific interaction represented : $$\\theta_{ij}^{(g)} = \\phi_{ij} + \\delta_{ij}^{(g)},$$ \\(\\phi_{ij}\\) baseline effect \\(\\delta_{ij}^{(g)}\\) group differences constrained sum zero.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"ordinal-variables","dir":"Reference","previous_headings":"","what":"Ordinal Variables","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"Regular ordinal variables: category thresholds decomposed baseline plus group differences category. Blume–Capel variables: category thresholds quadratic category index, linear quadratic terms split baseline plus group differences.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"variable-selection","dir":"Reference","previous_headings":"","what":"Variable Selection","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"difference_selection = TRUE, spike--slab priors applied difference parameters: Bernoulli: fixed prior inclusion probability. Beta–Bernoulli: inclusion probability given Beta prior.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"sampling-algorithms-and-warmup","dir":"Reference","previous_headings":"","what":"Sampling Algorithms and Warmup","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"Parameters updated within Gibbs framework, using sampling algorithms staged warmup scheme described bgm: Adaptive Metropolis–Hastings: componentwise random–walk     proposals Robbins–Monro adaptation proposal SDs. Hamiltonian Monte Carlo (HMC): joint updates fixed     leapfrog trajectories; step size optionally mass matrix     adapted warmup. –U–Turn Sampler (NUTS): adaptive HMC variant     dynamic trajectory lengths; warmup uses staged adaptation     schedule HMC. details staged adaptation schedule (fast–slow–fast phases), see bgm. addition, difference_selection = TRUE, updates inclusion indicators delayed late warmup. HMC/NUTS, appends two extra phases (Stage-3b Stage-3c), total number warmup iterations exceeds user-specified warmup. warmup, adaptation disabled: step size mass matrix fixed learned values, proposal SDs remain constant.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"Marsman M, van den Bergh D, Haslbeck JMB (2025). “Bayesian analysis ordinal Markov random field.” Psychometrika, 90(1), 146–182. doi:10.1017/psy.2024.4 . Marsman M, Waldorp LJ, Sekulovski N, Haslbeck JMB (2025). “Bayes factor tests group differences ordinal binary graphical models.” Psychometrika, 90(5), 1809–1842. doi:10.1017/psy.2025.10060 .","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgmCompare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Bayesian Estimation and Variable Selection for Group Differences in Markov Random Fields — bgmCompare","text":"","code":"# \\dontrun{ # Run bgmCompare on subset of the Boredom dataset x = Boredom[Boredom$language == \"fr\", 2:6] y = Boredom[Boredom$language != \"fr\", 2:6]  fit <- bgmCompare(x, y) #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 50/2000 (2.5%) #> Chain 2 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 55/2000 (2.8%) #> Chain 3 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 54/2000 (2.7%) #> Chain 4 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 52/2000 (2.6%) #> Total   (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 211/8000 (2.6%) #> Elapsed: 12s | ETA: 7m 22s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2000 (5.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2000 (5.0%) #> Chain 3 (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 161/2000 (8.1%) #> Chain 4 (Warmup): ⦗━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 123/2000 (6.2%) #> Total   (Warmup): ⦗━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 484/8000 (6.0%) #> Elapsed: 26s | ETA: 6m 43s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 150/2000 (7.5%) #> Chain 2 (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 162/2000 (8.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 232/2000 (11.6%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 181/2000 (9.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 725/8000 (9.1%) #> Elapsed: 27s | ETA: 4m 30s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 200/2000 (10.0%) #> Chain 2 (Warmup): ⦗━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 221/2000 (11.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 280/2000 (14.0%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 246/2000 (12.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 947/8000 (11.8%) #> Elapsed: 28s | ETA: 3m 28s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 250/2000 (12.5%) #> Chain 2 (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 260/2000 (13.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 329/2000 (16.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 278/2000 (13.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1117/8000 (14.0%) #> Elapsed: 29s | ETA: 2m 58s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 300/2000 (15.0%) #> Chain 2 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 317/2000 (15.8%) #> Chain 3 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 411/2000 (20.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 333/2000 (16.7%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1361/8000 (17.0%) #> Elapsed: 30s | ETA: 2m 26s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 350/2000 (17.5%) #> Chain 2 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 371/2000 (18.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 463/2000 (23.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 382/2000 (19.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1566/8000 (19.6%) #> Elapsed: 30s | ETA: 2m 3s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 400/2000 (20.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 422/2000 (21.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 505/2000 (25.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 435/2000 (21.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1762/8000 (22.0%) #> Elapsed: 31s | ETA: 1m 49s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 450/2000 (22.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 460/2000 (23.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 555/2000 (27.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 470/2000 (23.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1935/8000 (24.2%) #> Elapsed: 32s | ETA: 1m 40s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 500/2000 (25.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 534/2000 (26.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 637/2000 (31.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 540/2000 (27.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2211/8000 (27.6%) #> Elapsed: 33s | ETA: 1m 26s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 550/2000 (27.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 598/2000 (29.9%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 698/2000 (34.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 611/2000 (30.6%) #> Total   (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2457/8000 (30.7%) #> Elapsed: 34s | ETA: 1m 16s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 600/2000 (30.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 652/2000 (32.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 756/2000 (37.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 665/2000 (33.2%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2673/8000 (33.4%) #> Elapsed: 34s | ETA: 1m 7s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 650/2000 (32.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 702/2000 (35.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 804/2000 (40.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 723/2000 (36.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2879/8000 (36.0%) #> Elapsed: 35s | ETA: 1m 2s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 700/2000 (35.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 763/2000 (38.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 848/2000 (42.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 787/2000 (39.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3098/8000 (38.7%) #> Elapsed: 36s | ETA: 57s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 750/2000 (37.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 810/2000 (40.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 877/2000 (43.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 830/2000 (41.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 3267/8000 (40.8%) #> Elapsed: 37s | ETA: 54s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 800/2000 (40.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 844/2000 (42.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 907/2000 (45.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 869/2000 (43.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 3420/8000 (42.8%) #> Elapsed: 37s | ETA: 50s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 850/2000 (42.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 888/2000 (44.4%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 955/2000 (47.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 913/2000 (45.6%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 3606/8000 (45.1%) #> Elapsed: 38s | ETA: 46s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 900/2000 (45.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 936/2000 (46.8%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1042/2000 (52.1%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 977/2000 (48.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 3855/8000 (48.2%) #> Elapsed: 39s | ETA: 42s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 950/2000 (47.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1016/2000 (50.8%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1138/2000 (56.9%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1069/2000 (53.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4173/8000 (52.2%) #> Elapsed: 40s | ETA: 37s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1000/2000 (50.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1067/2000 (53.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1190/2000 (59.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 1117/2000 (55.9%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4374/8000 (54.7%) #> Elapsed: 41s | ETA: 34s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1050/2000 (52.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 1117/2000 (55.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1235/2000 (61.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1159/2000 (58.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4561/8000 (57.0%) #> Elapsed: 42s | ETA: 32s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1100/2000 (55.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1165/2000 (58.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1280/2000 (64.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1206/2000 (60.3%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4751/8000 (59.4%) #> Elapsed: 42s | ETA: 29s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1150/2000 (57.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1210/2000 (60.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1320/2000 (66.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1256/2000 (62.8%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4936/8000 (61.7%) #> Elapsed: 43s | ETA: 27s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1200/2000 (60.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1261/2000 (63.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1377/2000 (68.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1307/2000 (65.3%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5145/8000 (64.3%) #> Elapsed: 44s | ETA: 24s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1250/2000 (62.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1311/2000 (65.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1429/2000 (71.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1360/2000 (68.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5350/8000 (66.9%) #> Elapsed: 44s | ETA: 22s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1300/2000 (65.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1370/2000 (68.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1490/2000 (74.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1417/2000 (70.9%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5577/8000 (69.7%) #> Elapsed: 45s | ETA: 20s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1350/2000 (67.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1418/2000 (70.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1540/2000 (77.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1467/2000 (73.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5775/8000 (72.2%) #> Elapsed: 46s | ETA: 18s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1400/2000 (70.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1466/2000 (73.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1586/2000 (79.3%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1512/2000 (75.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5964/8000 (74.6%) #> Elapsed: 46s | ETA: 16s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1450/2000 (72.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1519/2000 (75.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1639/2000 (82.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1562/2000 (78.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6170/8000 (77.1%) #> Elapsed: 47s | ETA: 14s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1500/2000 (75.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1571/2000 (78.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1693/2000 (84.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 1616/2000 (80.8%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6380/8000 (79.8%) #> Elapsed: 47s | ETA: 12s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1550/2000 (77.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1628/2000 (81.4%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1752/2000 (87.6%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1671/2000 (83.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 6601/8000 (82.5%) #> Elapsed: 48s | ETA: 10s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1600/2000 (80.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1673/2000 (83.7%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 1803/2000 (90.1%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1717/2000 (85.9%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6793/8000 (84.9%) #> Elapsed: 49s | ETA: 9s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1650/2000 (82.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1723/2000 (86.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 1853/2000 (92.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1769/2000 (88.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6995/8000 (87.4%) #> Elapsed: 49s | ETA: 7s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1700/2000 (85.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1768/2000 (88.4%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 1902/2000 (95.1%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 1817/2000 (90.8%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7186/8000 (89.8%) #> Elapsed: 50s | ETA: 6s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1750/2000 (87.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 1815/2000 (90.8%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1950/2000 (97.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 1868/2000 (93.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7383/8000 (92.3%) #> Elapsed: 51s | ETA: 4s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1800/2000 (90.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 1865/2000 (93.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 1919/2000 (96.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7584/8000 (94.8%) #> Elapsed: 51s | ETA: 3s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1850/2000 (92.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 1914/2000 (95.7%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7764/8000 (97.0%) #> Elapsed: 52s | ETA: 2s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1950/2000 (97.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7950/8000 (99.4%) #> Elapsed: 53s | ETA: 0s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8000/8000 (100.0%) #> Elapsed: 53s | ETA: 0s  # Posterior inclusion probabilities summary(fit)$indicator #>                            parameter    mean        sd        mcse #> 1                  loose_ends (main) 1.00000 0.0000000          NA #> 2    loose_ends-entertain (pairwise) 0.02325 0.1506965 0.002513170 #> 3   loose_ends-repetitive (pairwise) 0.03750 0.1899836 0.004256419 #> 4  loose_ends-stimulation (pairwise) 0.23725 0.4253968 0.017792724 #> 5    loose_ends-motivated (pairwise) 0.02825 0.1656863 0.003173629 #> 6                   entertain (main) 1.00000 0.0000000          NA #> 7    entertain-repetitive (pairwise) 0.02500 0.1561249 0.002991269 #> 8   entertain-stimulation (pairwise) 0.18875 0.3913099 0.018272075 #> 9     entertain-motivated (pairwise) 0.05150 0.2210153 0.006040147 #> 10                 repetitive (main) 1.00000 0.0000000          NA #> 11 repetitive-stimulation (pairwise) 0.02825 0.1656863 0.003953813 #> 12   repetitive-motivated (pairwise) 0.02575 0.1583886 0.003459813 #> 13                stimulation (main) 1.00000 0.0000000          NA #> 14  stimulation-motivated (pairwise) 0.01975 0.1391400 0.002360355 #> 15                  motivated (main) 1.00000 0.0000000          NA #>    n0->0 n0->1 n1->0 n1->1     n_eff     Rhat #> 1      0     0     0  3999        NA       NA #> 2   3820    86    86     7 3595.5275 1.002195 #> 3   3753    96    96    54 1992.2470 1.009272 #> 4   2869   181   181   768  571.6149 1.003953 #> 5   3797    89    89    24 2725.5904 1.001355 #> 6      0     0     0  3999        NA       NA #> 7   3820    79    79    21 2724.1673 1.010392 #> 8   3118   126   126   629  458.6337 1.035273 #> 9   3695    98    98   108 1338.9044 1.006512 #> 10     0     0     0  3999        NA       NA #> 11  3819    67    67    46 1756.0654 1.019624 #> 12  3827    69    69    34 2095.7647 1.005735 #> 13     0     0     0  3999        NA       NA #> 14  3848    72    72     7 3474.9572 1.007878 #> 15     0     0     0  3999        NA       NA  # Bayesian model averaged main effects for the groups coef(fit)$main_effects_groups #>                      group1     group2 #> loose_ends(c1)  -0.94847916 -0.9122637 #> loose_ends(c2)  -2.73830005 -2.2421801 #> loose_ends(c3)  -4.00344094 -3.5492671 #> loose_ends(c4)  -5.30565414 -4.8263594 #> loose_ends(c5)  -7.60490439 -7.4173428 #> loose_ends(c6)  -9.83631448 -9.9515591 #> entertain(c1)   -0.74846395 -1.0402382 #> entertain(c2)   -2.19691894 -2.2779922 #> entertain(c3)   -3.99440945 -3.6874009 #> entertain(c4)   -5.06514258 -5.1697063 #> entertain(c5)   -7.04162816 -6.9714915 #> entertain(c6)   -9.70054598 -9.4572750 #> repetitive(c1)  -0.04480446 -0.2810117 #> repetitive(c2)  -0.49229738 -0.9175164 #> repetitive(c3)  -1.02703129 -1.1326658 #> repetitive(c4)  -1.95387011 -1.7246345 #> repetitive(c5)  -3.55433935 -2.9664282 #> repetitive(c6)  -5.27816180 -4.6808262 #> stimulation(c1) -0.35178497 -0.8636060 #> stimulation(c2) -1.76146679 -1.8631979 #> stimulation(c3) -2.44639836 -2.6889364 #> stimulation(c4) -3.42891216 -3.8730976 #> stimulation(c5) -5.06067416 -5.3145492 #> stimulation(c6) -6.72852813 -7.4187688 #> motivated(c1)   -0.46033522 -0.7031279 #> motivated(c2)   -1.74435013 -1.8689496 #> motivated(c3)   -3.42496435 -3.1610737 #> motivated(c4)   -5.05738355 -4.5883487 #> motivated(c5)   -6.63058052 -6.6904899 #> motivated(c6)   -9.30833985 -8.9107323  # Bayesian model averaged pairwise effects for the groups coef(fit)$pairwise_effects_groups #>                            group1     group2 #> loose_ends-entertain   0.16928140 0.16943372 #> loose_ends-repetitive  0.05635862 0.05736773 #> loose_ends-stimulation 0.12186375 0.13363751 #> loose_ends-motivated   0.14044489 0.13993641 #> entertain-repetitive   0.06416191 0.06463843 #> entertain-stimulation  0.10462910 0.11296556 #> entertain-motivated    0.08418867 0.08578870 #> repetitive-stimulation 0.05610268 0.05680018 #> repetitive-motivated   0.13478409 0.13531828 #> stimulation-motivated  0.10789868 0.10822459 # }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":null,"dir":"Reference","previous_headings":"","what":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"R package bgms provides tools Bayesian analysis ordinal Markov random field (MRF), graphical model describing networks binary /ordinal variables (Marsman et al. 2025) . likelihood approximated via pseudolikelihood, Markov chain Monte Carlo (MCMC) methods used sample corresponding pseudoposterior distribution model parameters. main entry points : bgm: estimation one-sample design. bgmCompare: estimation group comparison         independent-sample design. functions support Bayesian effect selection spike--slab priors. one-sample designs, bgm models presence absence   edges variables. Posterior inclusion probabilities quantify   plausibility edge can converted Bayes factors   conditional independence tests. bgm can also model communities (clusters) variables.   posterior distribution number clusters provides evidence   clustering (Sekulovski et al. 2025) . independent-sample designs, bgmCompare estimates group   differences edge weights category thresholds. Posterior inclusion   probabilities quantify evidence differences can converted   Bayes factors parameter equivalence tests   (Marsman et al. 2025) .","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":"tools","dir":"Reference","previous_headings":"","what":"Tools","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"package also provides: Simulation response data MRFs Gibbs sampler         (simulate_mrf). Posterior estimation edge selection one-sample designs         (bgm). Posterior estimation group-difference selection         independent-sample designs (bgmCompare).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":"vignettes","dir":"Reference","previous_headings":"","what":"Vignettes","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"tutorials worked examples, see: vignette(\"intro\", package = \"bgms\") — Getting started. vignette(\"comparison\", package = \"bgms\") — Model comparison. vignette(\"diagnostics\", package = \"bgms\") — Diagnostics         spike--slab summaries.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":"references","dir":"Reference","previous_headings":"","what":"References","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"Marsman M, van den Bergh D, Haslbeck JMB (2025). “Bayesian analysis ordinal Markov random field.” Psychometrika, 90(1), 146–182. doi:10.1017/psy.2024.4 . Marsman M, Waldorp LJ, Sekulovski N, Haslbeck JMB (2025). “Bayes factor tests group differences ordinal binary graphical models.” Psychometrika, 90(5), 1809–1842. doi:10.1017/psy.2025.10060 . Sekulovski N, Arena G, Haslbeck JMB, Huth KBS, Friel N, Marsman M (2025). “Stochastic Block Prior Clustering Graphical Models.” Retrieved https://osf.io/preprints/psyarxiv/29p3m_v1. OSF preprint.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/bgms-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"bgms: Bayesian Analysis of Networks of Binary and/or Ordinal Variables — bgms-package","text":"Maintainer: Maarten Marsman m.marsman@uva.nl (ORCID) Authors: Don van den Bergh (ORCID) contributors: Nikola Sekulovski (ORCID) [contributor] Giuseppe Arena (ORCID) [contributor] Laura Groot [contributor] Gali Geller [contributor]","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","title":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","text":"Returns posterior means raw parameters (baseline + differences) group-specific effects bgmCompare fit, well inclusion indicators.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","text":"","code":"# S3 method for class 'bgmCompare' coef(object, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","text":"object object class bgmCompare. ... Ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgmCompare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Coefficients from a bgmCompare Object — coef.bgmCompare","text":"list components: main_effects_raw Posterior means raw main-effect parameters   (variables x [baseline + differences]). pairwise_effects_raw Posterior means raw pairwise-effect parameters   (pairs x [baseline + differences]). main_effects_groups Posterior means group-specific main effects   (variables x groups), computed baseline plus projected differences. pairwise_effects_groups Posterior means group-specific pairwise effects   (pairs x groups), computed baseline plus projected differences. indicators Posterior mean inclusion probabilities symmetric matrix,   diagonals corresponding main effects -diagonals pairwise effects.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgms.html","id":null,"dir":"Reference","previous_headings":"","what":"Extract Coefficients from a bgms Object — coef.bgms","title":"Extract Coefficients from a bgms Object — coef.bgms","text":"Returns posterior mean thresholds, pairwise effects, edge inclusion indicators bgms model fit.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extract Coefficients from a bgms Object — coef.bgms","text":"","code":"# S3 method for class 'bgms' coef(object, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extract Coefficients from a bgms Object — coef.bgms","text":"object object class bgms. ... Ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/coef.bgms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Extract Coefficients from a bgms Object — coef.bgms","text":"list following components: main Posterior mean category threshold parameters. pairwise Posterior mean pairwise interaction matrix. indicator Posterior mean edge inclusion indicators (available).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/extractor_functions.html","id":null,"dir":"Reference","previous_headings":"","what":"Extractor Functions for bgms Objects — extractor_functions","title":"Extractor Functions for bgms Objects — extractor_functions","text":"Extractor Functions bgms Objects","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/extractor_functions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Extractor Functions for bgms Objects — extractor_functions","text":"","code":"extract_arguments(bgms_object)  # S3 method for class 'bgms' extract_arguments(bgms_object)  # S3 method for class 'bgmCompare' extract_arguments(bgms_object)  extract_indicators(bgms_object)  # S3 method for class 'bgms' extract_indicators(bgms_object)  # S3 method for class 'bgmCompare' extract_indicators(bgms_object)  extract_posterior_inclusion_probabilities(bgms_object)  # S3 method for class 'bgms' extract_posterior_inclusion_probabilities(bgms_object)  extract_sbm(bgms_object)  # S3 method for class 'bgms' extract_sbm(bgms_object)  # S3 method for class 'bgmCompare' extract_posterior_inclusion_probabilities(bgms_object)  extract_indicator_priors(bgms_object)  # S3 method for class 'bgms' extract_indicator_priors(bgms_object)  # S3 method for class 'bgmCompare' extract_indicator_priors(bgms_object)  extract_pairwise_interactions(bgms_object)  # S3 method for class 'bgms' extract_pairwise_interactions(bgms_object)  # S3 method for class 'bgmCompare' extract_pairwise_interactions(bgms_object)  extract_category_thresholds(bgms_object)  # S3 method for class 'bgms' extract_category_thresholds(bgms_object)  # S3 method for class 'bgmCompare' extract_category_thresholds(bgms_object)  extract_group_params(bgms_object)  # S3 method for class 'bgmCompare' extract_group_params(bgms_object)  extract_edge_indicators(bgms_object)  extract_pairwise_thresholds(bgms_object)  extract_rhat(bgms_object)  # S3 method for class 'bgms' extract_rhat(bgms_object)  # S3 method for class 'bgmCompare' extract_rhat(bgms_object)  extract_ess(bgms_object)  # S3 method for class 'bgms' extract_ess(bgms_object)  # S3 method for class 'bgmCompare' extract_ess(bgms_object)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/extractor_functions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Extractor Functions for bgms Objects — extractor_functions","text":"bgms_object object class `bgms` `bgmCompare`.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/extractor_functions.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Extractor Functions for bgms Objects — extractor_functions","text":"functions extract various components objects returned `bgm()` function, edge indicators, posterior inclusion probabilities, parameter summaries. Internally, indicator samples stored `$gamma` (pre-0.1.4, now defunct) `$indicator` (0.1.4–0.1.5, deprecated). **bgms 0.1.6.0**, stored `$raw_samples$indicator`. Posterior inclusion probabilities computed edge indicators. Internally, indicator samples stored `$gamma` (pre-0.1.4, now defunct) `$indicator` (0.1.4–0.1.5, deprecated). **bgms 0.1.6.0**, stored `$raw_samples$indicator`. Pairwise interactions previously stored `$pairwise_effects` (pre-0.1.4, now defunct) `$posterior_mean_pairwise` (0.1.4–0.1.5, deprecated). **bgms 0.1.6.0**, stored `$raw_samples$pairwise` (raw samples) `$posterior_summary_pairwise` (summaries). Category thresholds previously stored `$main_effects` (pre-0.1.4, now defunct) `$posterior_mean_main` (0.1.4–0.1.5, deprecated). **bgms 0.1.6.0**, stored `$posterior_summary_main`.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/extractor_functions.html","id":"functions","dir":"Reference","previous_headings":"","what":"Functions","title":"Extractor Functions for bgms Objects — extractor_functions","text":"- `extract_arguments()` – Extract model arguments - `extract_indicators()` – Get sampled edge indicators - `extract_posterior_inclusion_probabilities()` – Posterior edge inclusion probabilities - `extract_pairwise_interactions()` – Posterior mean pairwise interactions - `extract_category_thresholds()` – Posterior mean category thresholds - `extract_indicator_priors()` – Prior structure used edge indicators - `extract_sbm()` – Extract stochastic block model parameters (applicable) - `extract_rhat()` – Extract R-hat convergence diagnostics - `extract_ess()` – Extract effective sample size estimates","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":null,"dir":"Reference","previous_headings":"","what":"Sample observations from the ordinal MRF — mrfSampler","title":"Sample observations from the ordinal MRF — mrfSampler","text":"`r lifecycle::badge(\"deprecated\")` `mrfSampler()` renamed [simulate_mrf()] bgms 0.1.6.3 follow package's naming conventions.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Sample observations from the ordinal MRF — mrfSampler","text":"","code":"mrfSampler(   num_states,   num_variables,   num_categories,   pairwise,   main,   variable_type = \"ordinal\",   baseline_category,   iter = 1000,   seed = NULL )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Sample observations from the ordinal MRF — mrfSampler","text":"num_states number states ordinal MRF generated. num_variables number variables ordinal MRF. num_categories Either positive integer vector positive integers length num_variables. number response categories top base category: num_categories = 1 generates binary states. pairwise symmetric num_variables num_variables matrix pairwise interactions. -diagonal elements used. main num_variables max(num_categories) matrix category thresholds. elements row indicate thresholds variable . num_categories vector, first num_categories[] elements used row . Blume-Capel model used category thresholds variable , row requires two values (details ); first \\(\\alpha\\), linear contribution Blume-Capel model second \\(\\beta\\), quadratic contribution. variable_type kind variables simulated? Can single character string specifying variable type p variables vector character strings length p specifying type variable separately. Currently, bgm supports “ordinal” “blume-capel”. Binary variables automatically treated “ordinal”. Defaults variable_type = \"ordinal\". baseline_category integer vector length num_variables specifying baseline_category category used Blume-Capel model (details ). Can integer value 0 num_categories (num_categories[]). iter number iterations used Gibbs sampler. function provides last state Gibbs sampler output. default set 1e3. seed Optional integer seed reproducibility. NULL, seed generated R's random number generator (set.seed() can used calling function).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/mrfSampler.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Sample observations from the ordinal MRF — mrfSampler","text":"matrix simulated observations (see [simulate_mrf()]).","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Conditional Probabilities from a Fitted bgmCompare Model — predict.bgmCompare","title":"Predict Conditional Probabilities from a Fitted bgmCompare Model — predict.bgmCompare","text":"Computes conditional probability distributions one variables given observed values variables data, using group-specific parameters bgmCompare model.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Conditional Probabilities from a Fitted bgmCompare Model — predict.bgmCompare","text":"","code":"# S3 method for class 'bgmCompare' predict(   object,   newdata,   group,   variables = NULL,   type = c(\"probabilities\", \"response\"),   method = c(\"posterior-mean\"),   ... )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Conditional Probabilities from a Fitted bgmCompare Model — predict.bgmCompare","text":"object object class bgmCompare. newdata matrix data frame n rows p columns containing observed data. Must variables (columns) original data used fit model. group Integer specifying group's parameters use prediction (1 number groups). Required argument. variables variables predict. Can : character vector variable names integer vector column indices NULL (default) predict variables type Character string specifying type prediction: \"probabilities\" Return full conditional probability     distribution variable observation. \"response\" Return predicted category (mode     conditional distribution). method Character string specifying parameter estimates use: \"posterior-mean\" Use posterior mean parameters. ... Additional arguments (currently ignored).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgmCompare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Conditional Probabilities from a Fitted bgmCompare Model — predict.bgmCompare","text":"type = \"probabilities\": named list one element per predicted variable. element matrix n rows num_categories + 1 columns containing \\(P(X_j = c | X_{-j})\\) observation category. type = \"response\": matrix n rows length(variables) columns containing predicted categories.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgmCompare.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Conditional Probabilities from a Fitted bgmCompare Model — predict.bgmCompare","text":"Group-specific parameters obtained applying projection matrix convert baseline parameters differences group-level estimates. function computes conditional distribution target variables given observed values variables.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgmCompare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Conditional Probabilities from a Fitted bgmCompare Model — predict.bgmCompare","text":"","code":"# \\donttest{ # Fit a comparison model x <- Boredom[Boredom$language == \"fr\", 2:6] y <- Boredom[Boredom$language != \"fr\", 2:6] fit <- bgmCompare(x, y) #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 50/2000 (2.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 49/2000 (2.5%) #> Chain 3 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 55/2000 (2.8%) #> Chain 4 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 53/2000 (2.6%) #> Total   (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 207/8000 (2.6%) #> Elapsed: 12s | ETA: 7m 31s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2000 (5.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 96/2000 (4.8%) #> Chain 3 (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 153/2000 (7.6%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 94/2000 (4.7%) #> Total   (Warmup): ⦗━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 443/8000 (5.5%) #> Elapsed: 26s | ETA: 7m 23s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 150/2000 (7.5%) #> Chain 2 (Warmup): ⦗━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 123/2000 (6.2%) #> Chain 3 (Warmup): ⦗━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 213/2000 (10.7%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2000 (5.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 586/8000 (7.3%) #> Elapsed: 27s | ETA: 5m 41s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 200/2000 (10.0%) #> Chain 2 (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 174/2000 (8.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 277/2000 (13.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 149/2000 (7.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 800/8000 (10.0%) #> Elapsed: 28s | ETA: 4m 12s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 250/2000 (12.5%) #> Chain 2 (Warmup): ⦗━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 213/2000 (10.7%) #> Chain 3 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 322/2000 (16.1%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 197/2000 (9.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 982/8000 (12.3%) #> Elapsed: 29s | ETA: 3m 27s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 300/2000 (15.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 249/2000 (12.4%) #> Chain 3 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 374/2000 (18.7%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 240/2000 (12.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1163/8000 (14.5%) #> Elapsed: 30s | ETA: 2m 56s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 350/2000 (17.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 279/2000 (14.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 423/2000 (21.1%) #> Chain 4 (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 275/2000 (13.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1327/8000 (16.6%) #> Elapsed: 30s | ETA: 2m 30s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 400/2000 (20.0%) #> Chain 2 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 317/2000 (15.8%) #> Chain 3 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 465/2000 (23.2%) #> Chain 4 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 307/2000 (15.3%) #> Total   (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1489/8000 (18.6%) #> Elapsed: 31s | ETA: 2m 15s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 450/2000 (22.5%) #> Chain 2 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 365/2000 (18.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 515/2000 (25.8%) #> Chain 4 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 364/2000 (18.2%) #> Total   (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1694/8000 (21.2%) #> Elapsed: 32s | ETA: 1m 59s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 500/2000 (25.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 432/2000 (21.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 588/2000 (29.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 436/2000 (21.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1956/8000 (24.4%) #> Elapsed: 33s | ETA: 1m 41s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 550/2000 (27.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 466/2000 (23.3%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 638/2000 (31.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 472/2000 (23.6%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2126/8000 (26.6%) #> Elapsed: 34s | ETA: 1m 33s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 600/2000 (30.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 498/2000 (24.9%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 688/2000 (34.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 512/2000 (25.6%) #> Total   (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2298/8000 (28.7%) #> Elapsed: 34s | ETA: 1m 24s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 650/2000 (32.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 552/2000 (27.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 743/2000 (37.1%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 555/2000 (27.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2500/8000 (31.2%) #> Elapsed: 35s | ETA: 1m 17s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 700/2000 (35.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 591/2000 (29.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 795/2000 (39.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 598/2000 (29.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2684/8000 (33.6%) #> Elapsed: 36s | ETA: 1m 11s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 750/2000 (37.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 644/2000 (32.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 830/2000 (41.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 651/2000 (32.6%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2875/8000 (35.9%) #> Elapsed: 36s | ETA: 1m 4s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 800/2000 (40.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 694/2000 (34.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 858/2000 (42.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 699/2000 (34.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3051/8000 (38.1%) #> Elapsed: 37s | ETA: 1m #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 850/2000 (42.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 758/2000 (37.9%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 894/2000 (44.7%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 762/2000 (38.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 3264/8000 (40.8%) #> Elapsed: 38s | ETA: 55s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 900/2000 (45.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 836/2000 (41.8%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 945/2000 (47.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 837/2000 (41.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3518/8000 (44.0%) #> Elapsed: 39s | ETA: 50s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 950/2000 (47.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 891/2000 (44.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1024/2000 (51.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 887/2000 (44.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3752/8000 (46.9%) #> Elapsed: 40s | ETA: 45s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1000/2000 (50.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 922/2000 (46.1%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1074/2000 (53.7%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 916/2000 (45.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3912/8000 (48.9%) #> Elapsed: 41s | ETA: 43s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1050/2000 (52.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 953/2000 (47.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1129/2000 (56.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 943/2000 (47.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 4075/8000 (50.9%) #> Elapsed: 41s | ETA: 39s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1100/2000 (55.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1008/2000 (50.4%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1183/2000 (59.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 992/2000 (49.6%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 4283/8000 (53.5%) #> Elapsed: 42s | ETA: 36s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1150/2000 (57.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1064/2000 (53.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1239/2000 (62.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1050/2000 (52.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4503/8000 (56.3%) #> Elapsed: 43s | ETA: 33s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1200/2000 (60.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 1110/2000 (55.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1285/2000 (64.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 1105/2000 (55.2%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 4700/8000 (58.8%) #> Elapsed: 43s | ETA: 30s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1250/2000 (62.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1158/2000 (57.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1332/2000 (66.6%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1152/2000 (57.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 4892/8000 (61.2%) #> Elapsed: 44s | ETA: 28s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1300/2000 (65.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1211/2000 (60.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1381/2000 (69.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 1205/2000 (60.2%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 5097/8000 (63.7%) #> Elapsed: 45s | ETA: 26s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1350/2000 (67.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1262/2000 (63.1%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1431/2000 (71.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1255/2000 (62.7%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 5298/8000 (66.2%) #> Elapsed: 45s | ETA: 23s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1400/2000 (70.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1311/2000 (65.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1479/2000 (74.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1303/2000 (65.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 5493/8000 (68.7%) #> Elapsed: 46s | ETA: 21s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1450/2000 (72.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1361/2000 (68.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1529/2000 (76.4%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1352/2000 (67.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 5692/8000 (71.2%) #> Elapsed: 47s | ETA: 19s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1500/2000 (75.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1405/2000 (70.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1574/2000 (78.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1398/2000 (69.9%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 5877/8000 (73.5%) #> Elapsed: 47s | ETA: 17s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1550/2000 (77.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1451/2000 (72.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1628/2000 (81.4%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1451/2000 (72.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 6080/8000 (76.0%) #> Elapsed: 48s | ETA: 15s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1600/2000 (80.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1503/2000 (75.1%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1674/2000 (83.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1503/2000 (75.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 6280/8000 (78.5%) #> Elapsed: 48s | ETA: 13s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1650/2000 (82.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1546/2000 (77.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1725/2000 (86.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1557/2000 (77.8%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 6478/8000 (81.0%) #> Elapsed: 49s | ETA: 12s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1700/2000 (85.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1598/2000 (79.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1775/2000 (88.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 1615/2000 (80.8%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 6688/8000 (83.6%) #> Elapsed: 50s | ETA: 10s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1750/2000 (87.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1653/2000 (82.7%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1827/2000 (91.3%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1667/2000 (83.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 6897/8000 (86.2%) #> Elapsed: 50s | ETA: 8s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1800/2000 (90.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1704/2000 (85.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1876/2000 (93.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1719/2000 (86.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 7099/8000 (88.7%) #> Elapsed: 51s | ETA: 6s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1850/2000 (92.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1750/2000 (87.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 1920/2000 (96.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1771/2000 (88.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 7291/8000 (91.1%) #> Elapsed: 52s | ETA: 5s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1900/2000 (95.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1799/2000 (90.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 1970/2000 (98.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 1825/2000 (91.2%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 7494/8000 (93.7%) #> Elapsed: 52s | ETA: 4s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1950/2000 (97.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 1859/2000 (93.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1880/2000 (94.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 7689/8000 (96.1%) #> Elapsed: 53s | ETA: 2s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1944/2000 (97.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1927/2000 (96.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 7871/8000 (98.4%) #> Elapsed: 53s | ETA: 1s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8000/8000 (100.0%) #> Elapsed: 54s | ETA: 0s  # Predict conditional probabilities using group 1 parameters probs_g1 <- predict(fit, newdata = x[1:10, ], group = 1)  # Predict responses using group 2 parameters pred_g2 <- predict(fit, newdata = y[1:10, ], group = 2, type = \"response\") # }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgms.html","id":null,"dir":"Reference","previous_headings":"","what":"Predict Conditional Probabilities from a Fitted bgms Model — predict.bgms","title":"Predict Conditional Probabilities from a Fitted bgms Model — predict.bgms","text":"Computes conditional probability distributions one variables given observed values variables data.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Predict Conditional Probabilities from a Fitted bgms Model — predict.bgms","text":"","code":"# S3 method for class 'bgms' predict(   object,   newdata,   variables = NULL,   type = c(\"probabilities\", \"response\"),   method = c(\"posterior-mean\", \"posterior-sample\"),   ndraws = NULL,   seed = NULL,   ... )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Predict Conditional Probabilities from a Fitted bgms Model — predict.bgms","text":"object object class bgms. newdata matrix data frame n rows p columns containing observed data. Must variables (columns) original data used fit model. variables variables predict. Can : character vector variable names integer vector column indices NULL (default) predict variables type Character string specifying type prediction: \"probabilities\" Return full conditional probability     distribution variable observation. \"response\" Return predicted category (mode     conditional distribution). method Character string specifying parameter estimates use: \"posterior-mean\" Use posterior mean parameters. \"posterior-sample\" Average predictions posterior draws. ndraws Number posterior draws use method = \"posterior-sample\". NULL, uses available draws. seed Optional random seed reproducibility method = \"posterior-sample\". ... Additional arguments (currently ignored).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Predict Conditional Probabilities from a Fitted bgms Model — predict.bgms","text":"type = \"probabilities\": named list one element per predicted variable. element matrix n rows num_categories + 1 columns containing \\(P(X_j = c | X_{-j})\\) observation category. type = \"response\": matrix n rows length(variables) columns containing predicted categories. method = \"posterior-sample\", probabilities averaged posterior draws, attribute \"sd\" included containing standard deviation across draws.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgms.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Predict Conditional Probabilities from a Fitted bgms Model — predict.bgms","text":"observation, function computes conditional distribution target variable(s) given observed values variables. conditional distribution used internally Gibbs sampler.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/predict.bgms.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Predict Conditional Probabilities from a Fitted bgms Model — predict.bgms","text":"","code":"# \\donttest{ # Fit a model fit <- bgm(x = Wenchuan[, 1:5]) #> 7 rows with missing values excluded (n = 355 remaining). #> To impute missing values instead, use na_action = \"impute\". #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 50/2000 (2.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 48/2000 (2.4%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 37/2000 (1.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 41/2000 (2.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 176/8000 (2.2%) #> Elapsed: 1s | ETA: 44s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2000 (5.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 92/2000 (4.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 85/2000 (4.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 96/2000 (4.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 373/8000 (4.7%) #> Elapsed: 3s | ETA: 1m 1s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 200/2000 (10.0%) #> Chain 2 (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 162/2000 (8.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 96/2000 (4.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 178/2000 (8.9%) #> Total   (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 636/8000 (8.0%) #> Elapsed: 3s | ETA: 35s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 350/2000 (17.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 282/2000 (14.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 192/2000 (9.6%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 294/2000 (14.7%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1118/8000 (14.0%) #> Elapsed: 4s | ETA: 25s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 500/2000 (25.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 424/2000 (21.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 344/2000 (17.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 463/2000 (23.2%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1731/8000 (21.6%) #> Elapsed: 5s | ETA: 18s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 650/2000 (32.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 562/2000 (28.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 480/2000 (24.0%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 617/2000 (30.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2309/8000 (28.9%) #> Elapsed: 5s | ETA: 12s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 800/2000 (40.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 729/2000 (36.4%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 642/2000 (32.1%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 784/2000 (39.2%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2955/8000 (36.9%) #> Elapsed: 6s | ETA: 10s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 900/2000 (45.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 833/2000 (41.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 773/2000 (38.6%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 887/2000 (44.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3393/8000 (42.4%) #> Elapsed: 7s | ETA: 10s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1050/2000 (52.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 960/2000 (48.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 906/2000 (45.3%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1027/2000 (51.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3943/8000 (49.3%) #> Elapsed: 7s | ETA: 7s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1200/2000 (60.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1095/2000 (54.8%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1035/2000 (51.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1166/2000 (58.3%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 4496/8000 (56.2%) #> Elapsed: 8s | ETA: 6s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1350/2000 (67.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1239/2000 (62.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1165/2000 (58.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1312/2000 (65.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 5066/8000 (63.3%) #> Elapsed: 9s | ETA: 5s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1500/2000 (75.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1363/2000 (68.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1273/2000 (63.6%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1453/2000 (72.7%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 5589/8000 (69.9%) #> Elapsed: 9s | ETA: 4s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1650/2000 (82.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1504/2000 (75.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1394/2000 (69.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1583/2000 (79.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6131/8000 (76.6%) #> Elapsed: 10s | ETA: 3s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1800/2000 (90.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1634/2000 (81.7%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1525/2000 (76.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1731/2000 (86.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 6690/8000 (83.6%) #> Elapsed: 11s | ETA: 2s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1900/2000 (95.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1726/2000 (86.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1629/2000 (81.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1842/2000 (92.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 7097/8000 (88.7%) #> Elapsed: 11s | ETA: 1s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8000/8000 (100.0%) #> Elapsed: 13s | ETA: 0s  # Compute conditional probabilities for all variables probs <- predict(fit, newdata = Wenchuan[1:10, 1:5])  # Predict the first variable only probs_v1 <- predict(fit, newdata = Wenchuan[1:10, 1:5], variables = 1)  # Get predicted categories pred_class <- predict(fit, newdata = Wenchuan[1:10, 1:5], type = \"response\") # }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for `bgmCompare` objects — print.bgmCompare","title":"Print method for `bgmCompare` objects — print.bgmCompare","text":"Minimal console output `bgmCompare` fit objects.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for `bgmCompare` objects — print.bgmCompare","text":"","code":"# S3 method for class 'bgmCompare' print(x, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for `bgmCompare` objects — print.bgmCompare","text":"x object class `bgmCompare`. ... Ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgmCompare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for `bgmCompare` objects — print.bgmCompare","text":"Invisibly returns `x`.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgms.html","id":null,"dir":"Reference","previous_headings":"","what":"Print method for `bgms` objects — print.bgms","title":"Print method for `bgms` objects — print.bgms","text":"Minimal console output `bgms` fit objects.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Print method for `bgms` objects — print.bgms","text":"","code":"# S3 method for class 'bgms' print(x, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Print method for `bgms` objects — print.bgms","text":"x object class `bgms`. ... Ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/print.bgms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Print method for `bgms` objects — print.bgms","text":"Invisibly returns `x`.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate Data from a Fitted bgmCompare Model — simulate.bgmCompare","title":"Simulate Data from a Fitted bgmCompare Model — simulate.bgmCompare","text":"Generates new observations Markov Random Field model specified group using estimated parameters fitted bgmCompare object.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate Data from a Fitted bgmCompare Model — simulate.bgmCompare","text":"","code":"# S3 method for class 'bgmCompare' simulate(   object,   nsim = 500,   seed = NULL,   group,   method = c(\"posterior-mean\"),   iter = 1000,   ... )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate Data from a Fitted bgmCompare Model — simulate.bgmCompare","text":"object object class bgmCompare. nsim Number observations simulate. Default: 500. seed Optional random seed reproducibility. group Integer specifying group simulate (1 number groups). Required argument. method Character string specifying parameter estimates use: \"posterior-mean\" Use posterior mean parameters (faster,     single simulation). iter Number Gibbs iterations equilibration collecting samples. Default: 1000. ... Additional arguments (currently ignored).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgmCompare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate Data from a Fitted bgmCompare Model — simulate.bgmCompare","text":"matrix nsim rows p columns containing   simulated observations specified group.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgmCompare.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate Data from a Fitted bgmCompare Model — simulate.bgmCompare","text":"Group-specific parameters obtained applying projection matrix convert baseline parameters differences group-level estimates: group_param = baseline + projection[group, ] %*% differences. function uses group-specific interaction threshold parameters generate new data via Gibbs sampling.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgmCompare.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate Data from a Fitted bgmCompare Model — simulate.bgmCompare","text":"","code":"# \\donttest{ # Fit a comparison model x <- Boredom[Boredom$language == \"fr\", 2:6] y <- Boredom[Boredom$language != \"fr\", 2:6] fit <- bgmCompare(x, y) #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 50/2000 (2.5%) #> Chain 2 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 55/2000 (2.8%) #> Chain 3 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 51/2000 (2.5%) #> Chain 4 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 62/2000 (3.1%) #> Total   (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 218/8000 (2.7%) #> Elapsed: 11s | ETA: 6m 32s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2000 (5.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 150/2000 (7.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 126/2000 (6.3%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 280/2000 (14.0%) #> Total   (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 656/8000 (8.2%) #> Elapsed: 26s | ETA: 4m 51s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 150/2000 (7.5%) #> Chain 2 (Warmup): ⦗━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 221/2000 (11.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 199/2000 (10.0%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 380/2000 (19.0%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 950/8000 (11.9%) #> Elapsed: 27s | ETA: 3m 20s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 200/2000 (10.0%) #> Chain 2 (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 275/2000 (13.8%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 250/2000 (12.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 443/2000 (22.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1168/8000 (14.6%) #> Elapsed: 28s | ETA: 2m 43s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 250/2000 (12.5%) #> Chain 2 (Warmup): ⦗━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 321/2000 (16.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 299/2000 (14.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 490/2000 (24.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1360/8000 (17.0%) #> Elapsed: 29s | ETA: 2m 21s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 300/2000 (15.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 376/2000 (18.8%) #> Chain 3 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 360/2000 (18.0%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 548/2000 (27.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1584/8000 (19.8%) #> Elapsed: 30s | ETA: 2m 1s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 350/2000 (17.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 429/2000 (21.4%) #> Chain 3 (Warmup): ⦗━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 406/2000 (20.3%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 602/2000 (30.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1787/8000 (22.3%) #> Elapsed: 31s | ETA: 1m 47s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 400/2000 (20.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 476/2000 (23.8%) #> Chain 3 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 465/2000 (23.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 655/2000 (32.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1996/8000 (24.9%) #> Elapsed: 32s | ETA: 1m 36s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 450/2000 (22.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 516/2000 (25.8%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 502/2000 (25.1%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 708/2000 (35.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2176/8000 (27.2%) #> Elapsed: 32s | ETA: 1m 25s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 500/2000 (25.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 566/2000 (28.3%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 557/2000 (27.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 773/2000 (38.6%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2396/8000 (29.9%) #> Elapsed: 33s | ETA: 1m 17s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 550/2000 (27.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 614/2000 (30.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 613/2000 (30.6%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 826/2000 (41.3%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2603/8000 (32.5%) #> Elapsed: 34s | ETA: 1m 10s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 600/2000 (30.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 662/2000 (33.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 660/2000 (33.0%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 863/2000 (43.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2785/8000 (34.8%) #> Elapsed: 35s | ETA: 1m 5s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 650/2000 (32.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 710/2000 (35.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 715/2000 (35.8%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 890/2000 (44.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2965/8000 (37.1%) #> Elapsed: 35s | ETA: 59s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 700/2000 (35.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 760/2000 (38.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━⦘ 764/2000 (38.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 924/2000 (46.2%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3148/8000 (39.4%) #> Elapsed: 36s | ETA: 55s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 750/2000 (37.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 804/2000 (40.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 811/2000 (40.6%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 957/2000 (47.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3322/8000 (41.5%) #> Elapsed: 37s | ETA: 52s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 800/2000 (40.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 828/2000 (41.4%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 847/2000 (42.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 998/2000 (49.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 3473/8000 (43.4%) #> Elapsed: 37s | ETA: 48s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 850/2000 (42.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 873/2000 (43.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 886/2000 (44.3%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1064/2000 (53.2%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 3673/8000 (45.9%) #> Elapsed: 38s | ETA: 45s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 900/2000 (45.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 920/2000 (46.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 938/2000 (46.9%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1158/2000 (57.9%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 3916/8000 (48.9%) #> Elapsed: 39s | ETA: 41s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 950/2000 (47.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 973/2000 (48.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1020/2000 (51.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1248/2000 (62.4%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 4191/8000 (52.4%) #> Elapsed: 40s | ETA: 36s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1000/2000 (50.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1029/2000 (51.4%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1072/2000 (53.6%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1300/2000 (65.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 4401/8000 (55.0%) #> Elapsed: 41s | ETA: 34s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1050/2000 (52.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1078/2000 (53.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━⦘ 1124/2000 (56.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1353/2000 (67.7%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 4605/8000 (57.6%) #> Elapsed: 42s | ETA: 31s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1100/2000 (55.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1137/2000 (56.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1182/2000 (59.1%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1407/2000 (70.3%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 4826/8000 (60.3%) #> Elapsed: 42s | ETA: 28s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1150/2000 (57.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1197/2000 (59.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1236/2000 (61.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1465/2000 (73.2%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 5048/8000 (63.1%) #> Elapsed: 43s | ETA: 25s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1200/2000 (60.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━⦘ 1254/2000 (62.7%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1289/2000 (64.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1523/2000 (76.1%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 5266/8000 (65.8%) #> Elapsed: 44s | ETA: 23s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1250/2000 (62.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1306/2000 (65.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1340/2000 (67.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1573/2000 (78.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 5469/8000 (68.4%) #> Elapsed: 44s | ETA: 20s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1300/2000 (65.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 1358/2000 (67.9%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1388/2000 (69.4%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1627/2000 (81.3%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 5673/8000 (70.9%) #> Elapsed: 45s | ETA: 18s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1350/2000 (67.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1409/2000 (70.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1434/2000 (71.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1679/2000 (84.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 5872/8000 (73.4%) #> Elapsed: 46s | ETA: 17s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1400/2000 (70.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1463/2000 (73.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1484/2000 (74.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1727/2000 (86.4%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 6074/8000 (75.9%) #> Elapsed: 46s | ETA: 15s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1450/2000 (72.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 1508/2000 (75.4%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1532/2000 (76.6%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1780/2000 (89.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 6270/8000 (78.4%) #> Elapsed: 47s | ETA: 13s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1500/2000 (75.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1561/2000 (78.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1578/2000 (78.9%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1833/2000 (91.6%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 6472/8000 (80.9%) #> Elapsed: 47s | ETA: 11s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1550/2000 (77.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 1611/2000 (80.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1634/2000 (81.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1886/2000 (94.3%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 6681/8000 (83.5%) #> Elapsed: 48s | ETA: 9s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1600/2000 (80.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━⦘ 1661/2000 (83.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1689/2000 (84.5%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1940/2000 (97.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 6890/8000 (86.1%) #> Elapsed: 49s | ETA: 8s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1650/2000 (82.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1722/2000 (86.1%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1742/2000 (87.1%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7114/8000 (88.9%) #> Elapsed: 49s | ETA: 6s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1700/2000 (85.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━⦘ 1773/2000 (88.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1832/2000 (91.6%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7305/8000 (91.3%) #> Elapsed: 50s | ETA: 5s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1750/2000 (87.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━⦘ 1820/2000 (91.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 1917/2000 (95.9%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 7487/8000 (93.6%) #> Elapsed: 51s | ETA: 3s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1800/2000 (90.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 1869/2000 (93.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1999/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 7668/8000 (95.9%) #> Elapsed: 51s | ETA: 2s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1900/2000 (95.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 1973/2000 (98.7%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 7873/8000 (98.4%) #> Elapsed: 52s | ETA: 1s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8000/8000 (100.0%) #> Elapsed: 53s | ETA: 0s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8000/8000 (100.0%) #> Elapsed: 53s | ETA: 0s #> NUTS issues: #>   - Divergences: 1 (0.025%) - check R-hat and ESS   # Simulate 100 observations from group 1 new_data_g1 <- simulate(fit, nsim = 100, group = 1)  # Simulate 100 observations from group 2 new_data_g2 <- simulate(fit, nsim = 100, group = 2) # }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgms.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate Data from a Fitted bgms Model — simulate.bgms","title":"Simulate Data from a Fitted bgms Model — simulate.bgms","text":"Generates new observations Markov Random Field model using estimated parameters fitted bgms object.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate Data from a Fitted bgms Model — simulate.bgms","text":"","code":"# S3 method for class 'bgms' simulate(   object,   nsim = 500,   seed = NULL,   method = c(\"posterior-mean\", \"posterior-sample\"),   ndraws = NULL,   iter = 1000,   cores = parallel::detectCores(),   display_progress = c(\"per-chain\", \"total\", \"none\"),   ... )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate Data from a Fitted bgms Model — simulate.bgms","text":"object object class bgms. nsim Number observations simulate. Default: 500. seed Optional random seed reproducibility. method Character string specifying parameter estimates use: \"posterior-mean\" Use posterior mean parameters (faster,     single simulation). \"posterior-sample\" Sample posterior draws, producing     one dataset per draw (accounts parameter uncertainty). method     uses parallel processing cores > 1. ndraws Number posterior draws use method = \"posterior-sample\". NULL, uses available draws. iter Number Gibbs iterations equilibration collecting samples. Default: 1000. cores Number CPU cores parallel execution method = \"posterior-sample\". Default: parallel::detectCores(). display_progress Character string specifying type progress bar. Options: \"per-chain\", \"total\", \"none\". Default: \"per-chain\". ... Additional arguments (currently ignored).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate Data from a Fitted bgms Model — simulate.bgms","text":"method = \"posterior-mean\": matrix nsim rows p columns containing simulated observations. method = \"posterior-sample\": list matrices, one per posterior draw, nsim rows p columns.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgms.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate Data from a Fitted bgms Model — simulate.bgms","text":"function uses estimated interaction threshold parameters generate new data via Gibbs sampling. method = \"posterior-sample\", parameter uncertainty propagated simulated data using different posterior draws. Parallel processing available method via cores argument.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate.bgms.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate Data from a Fitted bgms Model — simulate.bgms","text":"","code":"# \\donttest{ # Fit a model fit <- bgm(x = Wenchuan[, 1:5]) #> 7 rows with missing values excluded (n = 355 remaining). #> To impute missing values instead, use na_action = \"impute\". #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 50/2000 (2.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 39/2000 (1.9%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 42/2000 (2.1%) #> Chain 4 (Warmup): ⦗━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 55/2000 (2.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 186/8000 (2.3%) #> Elapsed: 1s | ETA: 42s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 100/2000 (5.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 82/2000 (4.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 87/2000 (4.3%) #> Chain 4 (Warmup): ⦗━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 109/2000 (5.5%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 378/8000 (4.7%) #> Elapsed: 3s | ETA: 1m #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 200/2000 (10.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 97/2000 (4.9%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 137/2000 (6.9%) #> Chain 4 (Warmup): ⦗━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 207/2000 (10.3%) #> Total   (Warmup): ⦗━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 641/8000 (8.0%) #> Elapsed: 3s | ETA: 34s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 300/2000 (15.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 185/2000 (9.2%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 245/2000 (12.2%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 335/2000 (16.8%) #> Total   (Warmup): ⦗━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1065/8000 (13.3%) #> Elapsed: 4s | ETA: 26s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 400/2000 (20.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 280/2000 (14.0%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 326/2000 (16.3%) #> Chain 4 (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 460/2000 (23.0%) #> Total   (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1466/8000 (18.3%) #> Elapsed: 4s | ETA: 18s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 500/2000 (25.0%) #> Chain 2 (Warmup): ⦗━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 374/2000 (18.7%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 439/2000 (21.9%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 561/2000 (28.1%) #> Total   (Warmup): ⦗━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1874/8000 (23.4%) #> Elapsed: 5s | ETA: 16s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 650/2000 (32.5%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 511/2000 (25.6%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 553/2000 (27.7%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 715/2000 (35.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2429/8000 (30.4%) #> Elapsed: 5s | ETA: 11s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 800/2000 (40.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 642/2000 (32.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 689/2000 (34.4%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 843/2000 (42.1%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2974/8000 (37.2%) #> Elapsed: 6s | ETA: 10s #> Chain 1 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 900/2000 (45.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━━⦘ 809/2000 (40.5%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 831/2000 (41.5%) #> Chain 4 (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 956/2000 (47.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━━⦘ 3496/8000 (43.7%) #> Elapsed: 7s | ETA: 9s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1000/2000 (50.0%) #> Chain 2 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 903/2000 (45.1%) #> Chain 3 (Warmup): ⦗━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━━⦘ 924/2000 (46.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 1056/2000 (52.8%) #> Total   (Warmup): ⦗━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━━⦘ 3883/8000 (48.5%) #> Elapsed: 7s | ETA: 7s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1100/2000 (55.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1004/2000 (50.2%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━━⦘ 1023/2000 (51.1%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1154/2000 (57.7%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━━━⦘ 4281/8000 (53.5%) #> Elapsed: 8s | ETA: 7s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1250/2000 (62.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1153/2000 (57.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━━⦘ 1164/2000 (58.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1291/2000 (64.5%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━━━⦘ 4858/8000 (60.7%) #> Elapsed: 9s | ETA: 6s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1400/2000 (70.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1291/2000 (64.5%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━━⦘ 1315/2000 (65.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━⦘ 1425/2000 (71.2%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━━━⦘ 5431/8000 (67.9%) #> Elapsed: 9s | ETA: 4s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1550/2000 (77.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1434/2000 (71.7%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━━⦘ 1454/2000 (72.7%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1564/2000 (78.2%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━━⦘ 6002/8000 (75.0%) #> Elapsed: 10s | ETA: 3s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1700/2000 (85.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━━⦘ 1573/2000 (78.6%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━━━⦘ 1601/2000 (80.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1693/2000 (84.7%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 6567/8000 (82.1%) #> Elapsed: 11s | ETA: 2s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1850/2000 (92.5%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━━━━⦘ 1708/2000 (85.4%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1745/2000 (87.2%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1836/2000 (91.8%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 7139/8000 (89.2%) #> Elapsed: 11s | ETA: 1s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 1846/2000 (92.3%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━━⦘ 1875/2000 (93.8%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺⦘ 1960/2000 (98.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╺━⦘ 7681/8000 (96.0%) #> Elapsed: 12s | ETA: 0s #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 2 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 3 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Chain 4 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 2000/2000 (100.0%) #> Total   (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 8000/8000 (100.0%) #> Elapsed: 13s | ETA: 0s  # Simulate 100 new observations using posterior means new_data <- simulate(fit, nsim = 100)  # Simulate with parameter uncertainty (10 datasets) new_data_list <- simulate(fit, nsim = 100, method = \"posterior-sample\", ndraws = 10) #> Chain 1 (Sampling): ⦗━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━⦘ 10/10 (100.0%) #> Elapsed: 0s | ETA: 0s  # Use parallel processing for faster simulation new_data_list <- simulate(fit, nsim = 100, method = \"posterior-sample\",                           ndraws = 100, cores = 4) #>  #> User interrupt detected. Exiting gracefully. It may take a few seconds before all chains are terminated. #> All chains terminated. # }"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate_mrf.html","id":null,"dir":"Reference","previous_headings":"","what":"Simulate Observations from an Ordinal MRF — simulate_mrf","title":"Simulate Observations from an Ordinal MRF — simulate_mrf","text":"`simulate_mrf()` generates observations ordinal Markov Random Field using Gibbs sampling user-specified parameters.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate_mrf.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Simulate Observations from an Ordinal MRF — simulate_mrf","text":"","code":"simulate_mrf(   num_states,   num_variables,   num_categories,   pairwise,   main,   variable_type = \"ordinal\",   baseline_category,   iter = 1000,   seed = NULL )"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate_mrf.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Simulate Observations from an Ordinal MRF — simulate_mrf","text":"num_states number states ordinal MRF generated. num_variables number variables ordinal MRF. num_categories Either positive integer vector positive integers length num_variables. number response categories top base category: num_categories = 1 generates binary states. pairwise symmetric num_variables num_variables matrix pairwise interactions. -diagonal elements used. main num_variables max(num_categories) matrix category thresholds. elements row indicate thresholds variable . num_categories vector, first num_categories[] elements used row . Blume-Capel model used category thresholds variable , row requires two values (details ); first \\(\\alpha\\), linear contribution Blume-Capel model second \\(\\beta\\), quadratic contribution. variable_type kind variables simulated? Can single character string specifying variable type p variables vector character strings length p specifying type variable separately. Currently, bgm supports “ordinal” “blume-capel”. Binary variables automatically treated “ordinal”. Defaults variable_type = \"ordinal\". baseline_category integer vector length num_variables specifying baseline_category category used Blume-Capel model (details ). Can integer value 0 num_categories (num_categories[]). iter number iterations used Gibbs sampler. function provides last state Gibbs sampler output. default set 1e3. seed Optional integer seed reproducibility. NULL, seed generated R's random number generator (set.seed() can used calling function).","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate_mrf.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Simulate Observations from an Ordinal MRF — simulate_mrf","text":"num_states num_variables matrix simulated states ordinal MRF.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate_mrf.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Simulate Observations from an Ordinal MRF — simulate_mrf","text":"Gibbs sampler initiated random values response options, proceeds simulating states variable logistic model using variable states predictor variables. two modeling options category thresholds. default option assumes category thresholds free, except first threshold set zero identification. user needs specify thresholds remaining response categories. option useful type ordinal variable gives user freedom specifying model. Blume-Capel option specifically designed ordinal variables special type baseline_category category, neutral category Likert scale. Blume-Capel model specifies following quadratic model threshold parameters: $$\\mu_{\\text{c}} = \\alpha \\times (\\text{c} - \\text{r}) + \\beta \\times (\\text{c} - \\text{r})^2,$$ \\(\\mu_{\\text{c}}\\) threshold category c (now includes zero), \\(\\alpha\\) offers linear trend across categories (increasing threshold values \\(\\alpha > 0\\) decreasing threshold values \\(\\alpha <0\\)), \\(\\beta < 0\\), offers increasing penalty responding category away baseline_category category r, \\(\\beta > 0\\) suggests preference responding baseline_category category.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/simulate_mrf.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Simulate Observations from an Ordinal MRF — simulate_mrf","text":"","code":"# Generate responses from a network of five binary and ordinal variables. num_variables = 5 num_categories = sample(1:5, size = num_variables, replace = TRUE)  Interactions = matrix(0, nrow = num_variables, ncol = num_variables) Interactions[2, 1] = Interactions[4, 1] = Interactions[3, 2] =   Interactions[5, 2] = Interactions[5, 4] = .25 Interactions = Interactions + t(Interactions) Main = matrix(0, nrow = num_variables, ncol = max(num_categories))  x = simulate_mrf(   num_states = 1e3,   num_variables = num_variables,   num_categories = num_categories,   pairwise = Pairwise,   main = Main ) #> Error: object 'Pairwise' not found  # Generate responses from a network of 2 ordinal and 3 Blume-Capel variables. num_variables = 5 num_categories = 4  Interactions = matrix(0, nrow = num_variables, ncol = num_variables) Interactions[2, 1] = Interactions[4, 1] = Interactions[3, 2] =   Interactions[5, 2] = Interactions[5, 4] = .25 Interactions = Interactions + t(Interactions)  Main = matrix(NA, num_variables, num_categories) Main[, 1] = -1 Main[, 2] = -1 Main[3, ] = sort(-abs(rnorm(4)), decreasing = TRUE) Main[5, ] = sort(-abs(rnorm(4)), decreasing = TRUE)  x = simulate_mrf(   num_states = 1e3,   num_variables = num_variables,   num_categories = num_categories,   pairwise = Pairwise,   main = Main,   variable_type = c(\"b\", \"b\", \"o\", \"b\", \"o\"),   baseline_category = 2 ) #> Error: object 'Pairwise' not found"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgmCompare.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary method for `bgmCompare` objects — summary.bgmCompare","title":"Summary method for `bgmCompare` objects — summary.bgmCompare","text":"Returns posterior summaries diagnostics fitted `bgmCompare` model.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgmCompare.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary method for `bgmCompare` objects — summary.bgmCompare","text":"","code":"# S3 method for class 'bgmCompare' summary(object, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgmCompare.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary method for `bgmCompare` objects — summary.bgmCompare","text":"object object class `bgmCompare`. ... Currently ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgmCompare.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary method for `bgmCompare` objects — summary.bgmCompare","text":"object class `summary.bgmCompare` posterior summaries.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgms.html","id":null,"dir":"Reference","previous_headings":"","what":"Summary method for `bgms` objects — summary.bgms","title":"Summary method for `bgms` objects — summary.bgms","text":"Returns posterior summaries diagnostics fitted `bgms` model.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgms.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Summary method for `bgms` objects — summary.bgms","text":"","code":"# S3 method for class 'bgms' summary(object, ...)"},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgms.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Summary method for `bgms` objects — summary.bgms","text":"object object class `bgms`. ... Currently ignored.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/reference/summary.bgms.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Summary method for `bgms` objects — summary.bgms","text":"object class `summary.bgms` posterior summaries.","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-6-3","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.6.3","text":"extract_rhat(): extract R-hat convergence diagnostics fitted objects extract_ess(): extract effective sample size estimates fitted objects verbose argument: control informational messages; set options(bgms.verbose = FALSE) suppress globally simulate_mrf(): standalone MRF simulation user-specified parameters simulate.bgms(): generate observations fitted models (supports parallel processing) predict.bgms(): compute conditional probabilities P(X_j | X_{-j}) main_difference_selection argument bgmCompare(): control threshold difference selection standardize argument: scale Cauchy prior response score range baseline_category now stored fitted object Blume-Capel simulation/prediction","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-6-3","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.6.3","text":"fixed matrix indexing posterior_mean_indicator: now correctly maps C++ row-major order R matrices (#77) fixed mass matrix adaptation: now correctly uses variance instead precision fixed step size heuristic: re-runs mass matrix updates, resamples momentum iteration fixed E-BFMI diagnostic: now uses actual accepted trajectory momentum fixed Blume-Capel interaction: uses centered scores (c - ref) pseudolikelihood denominator","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-6-3","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.6.3","text":"expanded test suite: input validation, extractor functions, S3 methods, simulation, numerical sanity tests improved warmup schedule: fixed buffers (75/25/50) proportional fallback short warmup edge selection warmup now within user budget: 85%/10%/5% split stages 1-3a/3b/3c streamlined user messages: concise warnings, consolidated NUTS diagnostics E-BFMI threshold adjusted 0.2 (standard)","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"deprecated-0-1-6-3","dir":"Changelog","previous_headings":"","what":"Deprecated","title":"bgms 0.1.6.3","text":"mrfSampler() → use simulate_mrf()","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-0162","dir":"Changelog","previous_headings":"","what":"bgms 0.1.6.2","title":"bgms 0.1.6.2","text":"CRAN release: 2026-01-20","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-6-2","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.6.2","text":"added option separately specify beta priors within- -cluster probabilities SBM prior.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-6-2","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.6.2","text":"reparameterized Blume-capel model use (score-baseline) instead score. implemented new way compute denominators probabilities. made computation faster stable. refactored c++ code better maintainability. removed prepared_data field bgm objects.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-6-2","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.6.2","text":"fixed numerical problems Blume-Capel variables using HMC NUTS. fixed reporting bug category thresholds ordinal variables single category incorrectly expanded two parameters, resulting spurious NA values.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-0161","dir":"Changelog","previous_headings":"","what":"bgms 0.1.6.1","title":"bgms 0.1.6.1","text":"CRAN release: 2025-10-04","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-6-1","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.6.1","text":"added extractor function joint SBM output cleaned documentation, c++ files changed length warmup phase warmup scheduler HMC / NUTS (15% → 7.5%)","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-6-1","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.6.1","text":"fixed problem warmup scheduling adaptive-metropolis bgmCompare() fixed stability problems parallel sampling bgm() fixed spurious output errors printing console user interrupt.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-0160","dir":"Changelog","previous_headings":"","what":"bgms 0.1.6.0","title":"bgms 0.1.6.0","text":"CRAN release: 2025-09-27","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-6-0","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.6.0","text":"added NUTS HMC options sampling bgm() bgmCompare() models added support running multiple chains parallel added user interrupt handling parallel sampling added Markov chain diagnostics (effective sample size R-hat) sampled parameters added summary(), print(), coef() methods fitted objects MCMC sampling bgm() bgmCompare() now reproducible seed argument specified","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-6-0","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.6.0","text":"improved progress bar parallel sampling summary() now integrates functionality old summary_SBM() removed options modeling main differences; main differences now always estimated selected, equivalent previous main_difference_model = \"collapse\" setting","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-6-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.6.0","text":"fixed --bounds error bgmCompare() handling missing data fixed bug SBM prior computation","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"deprecated-0-1-6-0","dir":"Changelog","previous_headings":"","what":"Deprecated","title":"bgms 0.1.6.0","text":"interaction_scale → use pairwise_scale burnin → use warmup save → longer needed (outputs returned default) threshold_alpha, threshold_beta → use main_alpha, main_beta main_difference_model (removed without replacement) reference_category → use baseline_category pairwise_difference_*, main_difference_* → use unified difference_* arguments pairwise_beta_bernoulli_*, main_beta_bernoulli_* → use unified beta_bernoulli_* arguments interaction_scale → use pairwise_scale threshold_alpha, threshold_beta → use main_alpha, main_beta burnin → use warmup save → longer needed extract_edge_indicators() → use extract_indicators() extract_pairwise_thresholds() → use extract_category_thresholds() $gamma (pre-0.1.4) $indicator (0.1.4–0.1.5) → replaced $raw_samples$indicator $main_effects (pre-0.1.4) $posterior_mean_main (0.1.4–0.1.5) → replaced $raw_samples$main (raw samples) $posterior_summary_main (summaries)","code":""},{"path":[]},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-5-0","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.5.0 (GitHub only)","text":"bgmCompare function now allows network comparison two groups. new summary_sbm function can used summarize output bgm function “Stochastic-Block” prior. Two new data sets included package: ADHD Boredom.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-5-0","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.5.0 (GitHub only)","text":"bgm function “Stochastic-Block” prior can now also return sampled allocations block probabilities, sample return number blocks. underlying R c++ functions received massive update improve efficiency maintainance. Repository moved Bayesian Graphical Modelling Lab organization. Included custom c++ implementations exp log Windows.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-5-0","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.5.0 (GitHub only)","text":"Fixed bug bgmCompare function selecting group differences blume-capel parameters. Parameter differences selected fixed zero still updated. Fixed bug bgmCompare function handling samples blume-capel parameters. Output properly stored. Fixed bug bgmCompare function handling threshold estimation missing categories main_model = “Free”. sufficient statistics number categories computed correctly. Partially fixed bug bgms package slower Windows Linux MacOS. computation exp log using gcc compiler Windows really slow. custom c++ implementation, speed now closer speed achieved Linux MacOS.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-0142","dir":"Changelog","previous_headings":"","what":"bgms 0.1.4.2","title":"bgms 0.1.4.2","text":"CRAN release: 2024-12-05","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bug-fixes-0-1-4-2","dir":"Changelog","previous_headings":"","what":"Bug fixes","title":"bgms 0.1.4.2","text":"fixed bug adjusting variance proposal distributions fixed bug recoding data “collapse” condition","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-4-2","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.4.2","text":"selection = TRUE, burnin phase now runs 2 * burnin iterations instead 1 * burnin. ensures chain starts well-calibrated parameter values changed maximum standard deviation adaptive proposal 20 back 2","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-0141","dir":"Changelog","previous_headings":"","what":"bgms 0.1.4.1","title":"bgms 0.1.4.1","text":"CRAN release: 2024-11-12 minor release adds documentation output bug fixes.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-014","dir":"Changelog","previous_headings":"","what":"bgms 0.1.4","title":"bgms 0.1.4","text":"CRAN release: 2024-10-20","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-4","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.4","text":"Comparing category threshold pairwise interaction parameters two independent samples bgmCompare(). Stochastic Block model new prior option network structure bgm().","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"other-changes-0-1-4","dir":"Changelog","previous_headings":"","what":"Other changes","title":"bgms 0.1.4","text":"Exported extractor functions extract results bgm objects safe way. Changed maximum standard deviation adaptive proposal 2 20. small bug fixes.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-013","dir":"Changelog","previous_headings":"","what":"bgms 0.1.3","title":"bgms 0.1.3","text":"CRAN release: 2024-02-25","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-3","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.3","text":"Added support Bayesian estimation without edge selection bgm(). Added support simulating data (mixed) binary, ordinal, Blume-Capel MRF mrfSampler() Added support analyzing (mixed) binary, ordinal, Blume-Capel variables bgm()","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"user-level-changes-0-1-3","dir":"Changelog","previous_headings":"","what":"User level changes","title":"bgms 0.1.3","text":"Removed support optimization based functions, mple(), mppe(), bgm.em() Removed support Unit-Information prior bgm() Removed support non-adaptive Metropolis bgm() Reduced file size saving raw MCMC samples","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-012","dir":"Changelog","previous_headings":"","what":"bgms 0.1.2","title":"bgms 0.1.2","text":"CRAN release: 2023-10-13 minor release adds bug fixes.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"bgms-011","dir":"Changelog","previous_headings":"","what":"bgms 0.1.1","title":"bgms 0.1.1","text":"CRAN release: 2023-09-01 minor release adding new features fixing minor bugs.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"new-features-0-1-1","dir":"Changelog","previous_headings":"","what":"New features","title":"bgms 0.1.1","text":"Missing data imputation bgm function. See na.action option. Prior distributions network structure bgm function. See edge_prior option. Adaptive Metropolis alternative current random walk Metropolis algorithm bgm function. See adaptive option.","code":""},{"path":"https://bayesian-graphical-modelling-lab.github.io/bgms/news/index.html","id":"user-level-changes-0-1-1","dir":"Changelog","previous_headings":"","what":"User level changes","title":"bgms 0.1.1","text":"Changed default specification interaction prior UnitInfo Cauchy. See interaction_prior option. Changed default threshold hyperparameter specification 1.0 0.5. See threshold_alpha threshold_beta options. Analysis output now uses column names data.","code":""}]
